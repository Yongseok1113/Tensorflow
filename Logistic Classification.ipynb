{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBinary Classification\\n둘 중 하나로 분류. \\nex)\\ndetect spam e-mail : Spam or Ham\\n\\nlinear regression 으로는 부적합한 케이스.\\n\\n예를 들어 pass, nonpass를 분류할 때 \\n\\n6시간 이상 공부한 사람은 pass라는 데이터가 있다고하면\\n\\n50시간 이상 공부한 사람이 pass인 것을 학습시킬 경우. hypothesis의 기울기가 점점 낮아진다. \\n\\n0과 1사이인 0.5 이상이면 합격인 알고리즘이라 할 때 더 긴 시간을 공부한 사람의 데이터를 학습시킬 수록 기울기가 낮아지게 되고\\n\\n6시간 이상이면서 0.5미만인 데이터가 생긴다. 결국 실제로는 합격인데 불합격을 예측해버리는 상황이 된다.\\n\\n이런 문제를 해결하기위해 특정 두개의 값으로 부드럽게 수렴하는 그래프를 찾게 되는데 이에 적합한 함수로 시그모이드 함수를 채택하게 된다.\\n\\n시그모이드 함수 g(z)는 g(Z) = 1 / (1 + e-Z) 로 \\n\\n좌표점 (0, 0.5)를 중심으로 음수로 갈수록 0에 수렴하고 양의 무한으로 갈 수록 1에 수렴하는 s자형 함수 이다.\\n\\nZ = W(transpose)X = 행렬곱(W, X)\\ng(z) = 1 / (1 + e-z)\\nH(x) = 1 / (1 + e-행렬곱(W,X))\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Binary Classification\n",
    "둘 중 하나로 분류. \n",
    "ex)\n",
    "detect spam e-mail : Spam or Ham\n",
    "\n",
    "linear regression 으로는 부적합한 케이스.\n",
    "\n",
    "예를 들어 pass, nonpass를 분류할 때 \n",
    "\n",
    "6시간 이상 공부한 사람은 pass라는 데이터가 있다고하면\n",
    "\n",
    "50시간 이상 공부한 사람이 pass인 것을 학습시킬 경우. hypothesis의 기울기가 점점 낮아진다. \n",
    "\n",
    "0과 1사이인 0.5 이상이면 합격인 알고리즘이라 할 때 더 긴 시간을 공부한 사람의 데이터를 학습시킬 수록 기울기가 낮아지게 되고\n",
    "\n",
    "6시간 이상이면서 0.5미만인 데이터가 생긴다. 결국 실제로는 합격인데 불합격을 예측해버리는 상황이 된다.\n",
    "\n",
    "이런 문제를 해결하기위해 특정 두개의 값으로 부드럽게 수렴하는 그래프를 찾게 되는데 이에 적합해 보이는 함수가 시그모이드 함수 이다.\n",
    "\n",
    "시그모이드 함수 g(z)는 g(Z) = 1 / (1 + e-Z) 로 \n",
    "\n",
    "좌표점 (0, 0.5)를 중심으로 음수로 갈수록 0에 수렴하고 양의 무한으로 갈 수록 1에 수렴하는 s자형 함수 이다.\n",
    "\n",
    "Z = W(transpose)X = 행렬곱(W, X)\n",
    "g(z) = 1 / (1 + e-z)\n",
    "H(x) = 1 / (1 + e-행렬곱(W,X))\n",
    "\n",
    "그런데 실제 값과 가설함수간 거리의 양수를 구하기 위해 시그모이드 함수를 제곱할 경우, 물결치는 제곱함수 모양이 된다.(non-convex)\n",
    "\n",
    "이 경우 그래프 중간중간 기울기가 0이되는 구간이 생기게 되고 시작 위치에 따라 결과값이 달라지는 문제가 발생하기 때문에 \n",
    "\n",
    "cost함수로 제곱함수를 사용할 수 없다.(국지적 최솟값 문제)\n",
    "\n",
    "cost함수는 실제값과 가설과의 오차를 나타낸 함수. \n",
    "기울기가 작아지는 방향으로 나아가는 알고리즘을 사용하고 있기 때문에 \n",
    "어떤 점에서 시작해도 0을 향해 '극점 하나로 부드럽게 수렴하는'(convex한) 그래프가 필요하다.\n",
    "\n",
    "0과 1 두 값에 대해 각각 convex한 그래프가 필요하다. \n",
    "\n",
    "결론부터 얘기하면\n",
    "\n",
    "cost(W) = 1/m시그마 c(H(x),y)\n",
    "\n",
    "c(H(x),y) = -log(H(x))   : y = 1\n",
    "            -log(1-H(x)) : y = 0\n",
    "\n",
    "그래프를 사용한다. \n",
    "\n",
    "-log(H(x))함수는 H(x)값이 0에 가까워질수록 무한에 수렴하고 H(x)값이 1에 가까워질 수록 0에 수렴하면서 1일 때 0이된다.\n",
    "\n",
    "즉, 실제값이 1일 때는 가설함수가 1에 부드럽게 수렴해갈 때 cost결과 값이 0에 부드럽게 수렴하게 되므로 원하는 함수가 된다.\n",
    "\n",
    "반대로 실제값이 0일 때는 가설함수가 0에 부드럽게 수렵해갈 때 cost결과 값이 0에 부드럽게 수렴해야 한다. \n",
    "따라서 -log(1-H(x)) 함수를 사용하면 원하는 결과를 얻게 된다.\n",
    "\n",
    "if문으로 cost함수를 표현할 수 있지만, 하나의 식으로 표현할 수 있다.\n",
    "\n",
    "c(H(x),y) = -ylog(H(x)) -(1-y)log(1-H(x)) \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 1.2855262756347656\n",
      "hypothesis: [0.13148618 0.17615408 0.23194394 0.2989905  0.37593165 0.45968884\n",
      " 0.5457868 ], predicted: [0. 0. 0. 0. 0. 0. 1.], accuracy: 0.2857142984867096\n",
      "step: 200, cost: 0.7818233370780945\n",
      "hypothesis: [0.65520334 0.67070425 0.68584245 0.7005954  0.71494347 0.72887\n",
      " 0.7423613 ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 400, cost: 0.7492929100990295\n",
      "hypothesis: [0.67264444 0.68058157 0.6884154  0.6961429  0.7037613  0.7112678\n",
      " 0.71866024], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 600, cost: 0.7187085747718811\n",
      "hypothesis: [0.6895366  0.69038814 0.6912384  0.6920875  0.69293517 0.69378155\n",
      " 0.6946266 ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 800, cost: 0.6899943947792053\n",
      "hypothesis: [0.70582443 0.70008326 0.6942786  0.68841153 0.68248343 0.6764955\n",
      " 0.6704492 ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 1000, cost: 0.663066565990448\n",
      "hypothesis: [0.72146463 0.70963013 0.6975037  0.6850955  0.6724174  0.6594826\n",
      " 0.6463056 ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 1200, cost: 0.6378360390663147\n",
      "hypothesis: [0.7364258  0.7189965  0.70088285 0.68211776 0.6627419  0.64280355\n",
      " 0.622358  ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 1400, cost: 0.6142102479934692\n",
      "hypothesis: [0.7506883  0.72815514 0.7043872  0.67945606 0.65345675 0.6265078\n",
      " 0.59875   ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 1600, cost: 0.5920954942703247\n",
      "hypothesis: [0.76424295 0.7370831  0.7079898  0.67708766 0.644557   0.6106338\n",
      " 0.5756047 ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 1800, cost: 0.5713979601860046\n",
      "hypothesis: [0.7770898  0.7457625  0.7116662  0.67499006 0.6360346  0.59520996\n",
      " 0.5530243 ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 2000, cost: 0.5520256757736206\n",
      "hypothesis: [0.7892372  0.7541801  0.71539444 0.6731415  0.6278784  0.5802559\n",
      " 0.53109044], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 2200, cost: 0.5338891744613647\n",
      "hypothesis: [0.80070007 0.762326   0.7191548  0.6715208  0.6200758  0.5657836\n",
      " 0.5098656 ], predicted: [1. 1. 1. 1. 1. 1. 1.], accuracy: 0.5714285969734192\n",
      "step: 2400, cost: 0.5169026255607605\n",
      "hypothesis: [0.81149864 0.77019453 0.72293    0.67010826 0.6126125  0.5517984\n",
      " 0.48939455], predicted: [1. 1. 1. 1. 1. 1. 0.], accuracy: 0.7142857313156128\n",
      "step: 2600, cost: 0.500984251499176\n",
      "hypothesis: [0.8216573  0.7777831  0.7267053  0.66888523 0.6054735  0.53830016\n",
      " 0.4697067 ], predicted: [1. 1. 1. 1. 1. 1. 0.], accuracy: 0.7142857313156128\n",
      "step: 2800, cost: 0.48605674505233765\n",
      "hypothesis: [0.8312032  0.7850915  0.7304678  0.66783434 0.5986433  0.5252846\n",
      " 0.45081803], predicted: [1. 1. 1. 1. 1. 1. 0.], accuracy: 0.7142857313156128\n",
      "step: 3000, cost: 0.4720473885536194\n",
      "hypothesis: [0.8401655  0.79212224 0.7342065  0.6669396  0.5921066  0.5127437\n",
      " 0.43273306], predicted: [1. 1. 1. 1. 1. 1. 0.], accuracy: 0.7142857313156128\n",
      "step: 3200, cost: 0.4588882625102997\n",
      "hypothesis: [0.8485745  0.7988794  0.73791224 0.6661863  0.585848   0.5006669\n",
      " 0.41544718], predicted: [1. 1. 1. 1. 1. 1. 0.], accuracy: 0.7142857313156128\n",
      "step: 3400, cost: 0.44651609659194946\n",
      "hypothesis: [0.85646075 0.80536854 0.7415774  0.66556084 0.5798523  0.48904145\n",
      " 0.39894825], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 3600, cost: 0.4348723888397217\n",
      "hypothesis: [0.8638549  0.8115965  0.7451958  0.6650511  0.57410526 0.47785327\n",
      " 0.38321823], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 3800, cost: 0.4239029288291931\n",
      "hypothesis: [0.87078667 0.81757104 0.74876237 0.6646459  0.5685929  0.467087\n",
      " 0.36823466], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 4000, cost: 0.4135574996471405\n",
      "hypothesis: [0.8772854  0.8233007  0.7522731  0.6643351  0.5633019  0.4567271\n",
      " 0.35397205], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 4200, cost: 0.40379005670547485\n",
      "hypothesis: [0.883379   0.82879394 0.7557249  0.66410947 0.5582199  0.44675753\n",
      " 0.34040242], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 4400, cost: 0.39455822110176086\n",
      "hypothesis: [0.88909405 0.83405995 0.7591156  0.6639608  0.5533348  0.4371621\n",
      " 0.32749662], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 4600, cost: 0.38582295179367065\n",
      "hypothesis: [0.8944559  0.83910775 0.76244336 0.66388154 0.5486353  0.42792466\n",
      " 0.31522462], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 4800, cost: 0.377548485994339\n",
      "hypothesis: [0.89948833 0.8439466  0.76570725 0.6638651  0.54411113 0.4190298\n",
      " 0.30355638], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 5000, cost: 0.3697018623352051\n",
      "hypothesis: [0.9042139  0.84858525 0.7689064  0.6639051  0.539752   0.4104618\n",
      " 0.29246178], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 5200, cost: 0.3622528612613678\n",
      "hypothesis: [0.90865386 0.85303307 0.7720411  0.6639964  0.5355492  0.40220624\n",
      " 0.28191173], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 5400, cost: 0.35517382621765137\n",
      "hypothesis: [0.9128276  0.8572984  0.7751112  0.66413385 0.53149366 0.39424863\n",
      " 0.27187762], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 5600, cost: 0.3484393060207367\n",
      "hypothesis: [0.9167534  0.8613895  0.77811694 0.6643129  0.5275773  0.3865751\n",
      " 0.26233175], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 5800, cost: 0.34202590584754944\n",
      "hypothesis: [0.92044854 0.8653148  0.78105915 0.6645296  0.5237925  0.37917256\n",
      " 0.25324774], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 6000, cost: 0.33591195940971375\n",
      "hypothesis: [0.9239287  0.869082   0.78393865 0.66478026 0.5201324  0.3720284\n",
      " 0.24460015], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 6200, cost: 0.3300778269767761\n",
      "hypothesis: [0.9272086  0.8726984  0.7867562  0.6650617  0.51659    0.36513045\n",
      " 0.23636481], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 6400, cost: 0.3245052695274353\n",
      "hypothesis: [0.9303019  0.8761716  0.7895131  0.6653707  0.5131594  0.35846755\n",
      " 0.22851887], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 6600, cost: 0.3191774785518646\n",
      "hypothesis: [0.9332213  0.87950796 0.79221034 0.66570485 0.5098347  0.35202843\n",
      " 0.22104047], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 6800, cost: 0.3140789568424225\n",
      "hypothesis: [0.9359785  0.8827145  0.79484934 0.6660616  0.5066105  0.34580314\n",
      " 0.21390915], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 7000, cost: 0.30919548869132996\n",
      "hypothesis: [0.9385841  0.885797   0.7974311  0.6664386  0.50348157 0.33978143\n",
      " 0.2071053 ], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 7200, cost: 0.30451393127441406\n",
      "hypothesis: [0.9410486  0.88876164 0.7999573  0.66683406 0.50044346 0.33395448\n",
      " 0.20061097], predicted: [1. 1. 1. 1. 1. 0. 0.], accuracy: 0.8571428656578064\n",
      "step: 7400, cost: 0.30002209544181824\n",
      "hypothesis: [0.9433809  0.8916139  0.8024288  0.6672459  0.49749136 0.32831332\n",
      " 0.19440885], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 7600, cost: 0.2957088053226471\n",
      "hypothesis: [0.9455898  0.89435935 0.8048475  0.667673   0.4946218  0.32284978\n",
      " 0.18848285], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 7800, cost: 0.29156336188316345\n",
      "hypothesis: [0.94768316 0.89700264 0.8072142  0.6681131  0.49183005 0.31755546\n",
      " 0.18281738], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 8000, cost: 0.28757643699645996\n",
      "hypothesis: [0.94966847 0.8995488  0.8095306  0.66856545 0.48911288 0.31242338\n",
      " 0.17739853], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 8200, cost: 0.2837388515472412\n",
      "hypothesis: [0.9515525  0.90200245 0.8117978  0.6690284  0.48646685 0.3074462\n",
      " 0.1722127 ], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 8400, cost: 0.28004226088523865\n",
      "hypothesis: [0.9533418  0.9043678  0.8140173  0.6695012  0.48388863 0.3026172\n",
      " 0.16724722], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8600, cost: 0.276479035615921\n",
      "hypothesis: [0.955042   0.90664876 0.8161901  0.66998243 0.48137498 0.29793003\n",
      " 0.16249032], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 8800, cost: 0.27304190397262573\n",
      "hypothesis: [0.95665866 0.9088494  0.8183178  0.67047125 0.47892332 0.2933786\n",
      " 0.15793079], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 9000, cost: 0.26972413063049316\n",
      "hypothesis: [0.958197   0.9109734  0.82040167 0.67096734 0.47653124 0.2889577\n",
      " 0.15355855], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 9200, cost: 0.26651960611343384\n",
      "hypothesis: [0.9596616  0.9130242  0.82244265 0.6714693  0.47419575 0.28466144\n",
      " 0.14936352], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 9400, cost: 0.263422429561615\n",
      "hypothesis: [0.9610568  0.91500473 0.8244417  0.6719763  0.47191447 0.28048462\n",
      " 0.14533639], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 9600, cost: 0.2604270577430725\n",
      "hypothesis: [0.9623871  0.91691864 0.8264007  0.6724884  0.46968547 0.27642283\n",
      " 0.14146878], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 9800, cost: 0.2575284540653229\n",
      "hypothesis: [0.9636556  0.9187686  0.82832026 0.6730044  0.46750626 0.27247095\n",
      " 0.13775232], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n",
      "step: 10000, cost: 0.25472190976142883\n",
      "hypothesis: [0.9648664  0.9205574  0.83020145 0.673524   0.46537513 0.26862493\n",
      " 0.13417956], predicted: [1. 1. 1. 1. 0. 0. 0.], accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#one-variable Logistic Classification implement\n",
    "import tensorflow as tf\n",
    "\n",
    "#data set\n",
    "x_data = [10, 9, 8, 7, 6, 5, 4]\n",
    "y_data = [1, 1, 1, 1, 0, 0, 0]\n",
    "#node\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "#weight , bias\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "#hypothesis\n",
    "hypothesis = tf.sigmoid(W * X + b)\n",
    "#cost\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y) * tf.log(1 - hypothesis))\n",
    "\n",
    "#train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#Accuracy Computation\n",
    "#True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "#launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%200==0:\n",
    "            print('step: {0}, cost: {1}'.format(step, cost_val))\n",
    "            print('hypothesis: {0}, predicted: {1}, accuracy: {2}'.format(h, c, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 4.272256851196289\n",
      "hypothesis: [[0.02564198]\n",
      " [0.00279367]\n",
      " [0.01326007]\n",
      " [0.00040188]\n",
      " [0.00015226]\n",
      " [0.00020524]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]], accuracy: 0.5\n",
      "step: 200, cost: 0.5213462710380554\n",
      "hypothesis: [[0.349043  ]\n",
      " [0.34162384]\n",
      " [0.7388113 ]\n",
      " [0.60605365]\n",
      " [0.72594863]\n",
      " [0.89029926]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 400, cost: 0.49275708198547363\n",
      "hypothesis: [[0.29860228]\n",
      " [0.29968527]\n",
      " [0.74315715]\n",
      " [0.606411  ]\n",
      " [0.74512416]\n",
      " [0.9128274 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 600, cost: 0.47071513533592224\n",
      "hypothesis: [[0.26289147]\n",
      " [0.27263713]\n",
      " [0.7411318 ]\n",
      " [0.6083603 ]\n",
      " [0.7597445 ]\n",
      " [0.9257577 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 800, cost: 0.4520464241504669\n",
      "hypothesis: [[0.23642528]\n",
      " [0.2546444 ]\n",
      " [0.7352501 ]\n",
      " [0.6116213 ]\n",
      " [0.77174586]\n",
      " [0.93388367]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 1000, cost: 0.43537285923957825\n",
      "hypothesis: [[0.21591395]\n",
      " [0.24226022]\n",
      " [0.72674954]\n",
      " [0.6156443 ]\n",
      " [0.78190714]\n",
      " [0.9392852 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 1200, cost: 0.42004379630088806\n",
      "hypothesis: [[0.19940883]\n",
      " [0.23350385]\n",
      " [0.7164276 ]\n",
      " [0.6201407 ]\n",
      " [0.79076356]\n",
      " [0.9430502 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 1400, cost: 0.4057289659976959\n",
      "hypothesis: [[0.18570071]\n",
      " [0.22716758]\n",
      " [0.7048292 ]\n",
      " [0.62493914]\n",
      " [0.7986754 ]\n",
      " [0.94579023]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 1600, cost: 0.39224815368652344\n",
      "hypothesis: [[0.17400977]\n",
      " [0.2224792 ]\n",
      " [0.69234544]\n",
      " [0.62992746]\n",
      " [0.80588603]\n",
      " [0.9478704 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 1800, cost: 0.37949514389038086\n",
      "hypothesis: [[0.1638178 ]\n",
      " [0.21892688]\n",
      " [0.679268  ]\n",
      " [0.63502693]\n",
      " [0.8125601 ]\n",
      " [0.9495183 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 2000, cost: 0.3674004077911377\n",
      "hypothesis: [[0.1547705 ]\n",
      " [0.2161608 ]\n",
      " [0.66581947]\n",
      " [0.6401793 ]\n",
      " [0.8188098 ]\n",
      " [0.95088106]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 2200, cost: 0.35591399669647217\n",
      "hypothesis: [[0.14662111]\n",
      " [0.2139382 ]\n",
      " [0.6521722 ]\n",
      " [0.6453411 ]\n",
      " [0.82471263]\n",
      " [0.95205534]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 2400, cost: 0.3449963629245758\n",
      "hypothesis: [[0.13919449]\n",
      " [0.21208769]\n",
      " [0.63846123]\n",
      " [0.6504788 ]\n",
      " [0.8303219 ]\n",
      " [0.95310587]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 2600, cost: 0.33461377024650574\n",
      "hypothesis: [[0.1323638 ]\n",
      " [0.21048754]\n",
      " [0.62479216]\n",
      " [0.65556717]\n",
      " [0.8356755 ]\n",
      " [0.9540762 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 2800, cost: 0.3247359097003937\n",
      "hypothesis: [[0.12603596]\n",
      " [0.20905113]\n",
      " [0.6112468 ]\n",
      " [0.6605866 ]\n",
      " [0.84079975]\n",
      " [0.9549952 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 3000, cost: 0.31533491611480713\n",
      "hypothesis: [[0.1201413 ]\n",
      " [0.20771694]\n",
      " [0.59788954]\n",
      " [0.6655231 ]\n",
      " [0.8457148 ]\n",
      " [0.95588243]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 3200, cost: 0.30638405680656433\n",
      "hypothesis: [[0.11462629]\n",
      " [0.20644099]\n",
      " [0.58476764]\n",
      " [0.6703656 ]\n",
      " [0.8504348 ]\n",
      " [0.9567501 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 3400, cost: 0.2978588938713074\n",
      "hypothesis: [[0.10944989]\n",
      " [0.20519349]\n",
      " [0.5719184 ]\n",
      " [0.67510724]\n",
      " [0.85497147]\n",
      " [0.9576061 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 3600, cost: 0.2897355556488037\n",
      "hypothesis: [[0.104579  ]\n",
      " [0.2039536 ]\n",
      " [0.55936813]\n",
      " [0.67974305]\n",
      " [0.859334  ]\n",
      " [0.9584548 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 3800, cost: 0.28199148178100586\n",
      "hypothesis: [[0.0999867 ]\n",
      " [0.2027072 ]\n",
      " [0.54713464]\n",
      " [0.68426967]\n",
      " [0.8635302 ]\n",
      " [0.95929843]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 4000, cost: 0.274605393409729\n",
      "hypothesis: [[0.09565061]\n",
      " [0.2014457 ]\n",
      " [0.53523   ]\n",
      " [0.6886856 ]\n",
      " [0.86756676]\n",
      " [0.96013784]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 4200, cost: 0.2675569951534271\n",
      "hypothesis: [[0.09155172]\n",
      " [0.20016408]\n",
      " [0.52366126]\n",
      " [0.6929911 ]\n",
      " [0.87145054]\n",
      " [0.96097285]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 4400, cost: 0.26082730293273926\n",
      "hypothesis: [[0.08767354]\n",
      " [0.19886011]\n",
      " [0.51243156]\n",
      " [0.69718695]\n",
      " [0.8751874 ]\n",
      " [0.96180296]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 4600, cost: 0.2543981671333313\n",
      "hypothesis: [[0.08400127]\n",
      " [0.1975326 ]\n",
      " [0.50153923]\n",
      " [0.70127326]\n",
      " [0.8787819 ]\n",
      " [0.9626267 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 0.8333333134651184\n",
      "step: 4800, cost: 0.24825257062911987\n",
      "hypothesis: [[0.08052194]\n",
      " [0.19618249]\n",
      " [0.49098226]\n",
      " [0.70525324]\n",
      " [0.88224083]\n",
      " [0.96344304]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 5000, cost: 0.24237465858459473\n",
      "hypothesis: [[0.07722378]\n",
      " [0.1948117 ]\n",
      " [0.4807563 ]\n",
      " [0.7091293 ]\n",
      " [0.88556916]\n",
      " [0.9642508 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 5200, cost: 0.23674941062927246\n",
      "hypothesis: [[0.0740957 ]\n",
      " [0.1934216 ]\n",
      " [0.47085375]\n",
      " [0.7129029 ]\n",
      " [0.88877136]\n",
      " [0.9650486 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 5400, cost: 0.2313624769449234\n",
      "hypothesis: [[0.07112762]\n",
      " [0.19201478]\n",
      " [0.46126744]\n",
      " [0.7165773 ]\n",
      " [0.8918526 ]\n",
      " [0.96583533]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 5600, cost: 0.2262009233236313\n",
      "hypothesis: [[0.06831035]\n",
      " [0.19059396]\n",
      " [0.45198968]\n",
      " [0.72015584]\n",
      " [0.8948182 ]\n",
      " [0.96661   ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 5800, cost: 0.22125208377838135\n",
      "hypothesis: [[0.06563506]\n",
      " [0.18916121]\n",
      " [0.44301057]\n",
      " [0.72364044]\n",
      " [0.8976722 ]\n",
      " [0.96737146]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 6000, cost: 0.21650440990924835\n",
      "hypothesis: [[0.06309366]\n",
      " [0.18771964]\n",
      " [0.43432212]\n",
      " [0.72703516]\n",
      " [0.9004196 ]\n",
      " [0.9681193 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 6200, cost: 0.21194684505462646\n",
      "hypothesis: [[0.06067842]\n",
      " [0.18627134]\n",
      " [0.42591423]\n",
      " [0.73034257]\n",
      " [0.9030649 ]\n",
      " [0.9688529 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 6400, cost: 0.20756925642490387\n",
      "hypothesis: [[0.05838224]\n",
      " [0.1848186 ]\n",
      " [0.4177772 ]\n",
      " [0.7335652 ]\n",
      " [0.9056121 ]\n",
      " [0.96957135]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 6600, cost: 0.20336197316646576\n",
      "hypothesis: [[0.05619842]\n",
      " [0.18336368]\n",
      " [0.40990198]\n",
      " [0.7367064 ]\n",
      " [0.90806556]\n",
      " [0.9702746 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 6800, cost: 0.19931606948375702\n",
      "hypothesis: [[0.05412051]\n",
      " [0.18190876]\n",
      " [0.40227956]\n",
      " [0.7397694 ]\n",
      " [0.9104295 ]\n",
      " [0.97096235]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 7000, cost: 0.19542288780212402\n",
      "hypothesis: [[0.05214271]\n",
      " [0.18045542]\n",
      " [0.39489973]\n",
      " [0.74275595]\n",
      " [0.9127071 ]\n",
      " [0.9716343 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 7200, cost: 0.19167466461658478\n",
      "hypothesis: [[0.05025932]\n",
      " [0.17900515]\n",
      " [0.3877535 ]\n",
      " [0.74566877]\n",
      " [0.91490227]\n",
      " [0.97229034]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 7400, cost: 0.18806393444538116\n",
      "hypothesis: [[0.0484651 ]\n",
      " [0.17755994]\n",
      " [0.38083285]\n",
      " [0.74851125]\n",
      " [0.91701895]\n",
      " [0.97293055]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 7600, cost: 0.18458373844623566\n",
      "hypothesis: [[0.04675514]\n",
      " [0.17612109]\n",
      " [0.37412882]\n",
      " [0.75128555]\n",
      " [0.9190603 ]\n",
      " [0.97355485]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 7800, cost: 0.18122750520706177\n",
      "hypothesis: [[0.04512477]\n",
      " [0.17468971]\n",
      " [0.36763293]\n",
      " [0.7539938 ]\n",
      " [0.9210293 ]\n",
      " [0.97416353]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8000, cost: 0.17798908054828644\n",
      "hypothesis: [[0.04356965]\n",
      " [0.17326698]\n",
      " [0.36133716]\n",
      " [0.75663865]\n",
      " [0.9229295 ]\n",
      " [0.9747565 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 8200, cost: 0.17486262321472168\n",
      "hypothesis: [[0.04208562]\n",
      " [0.17185378]\n",
      " [0.3552342 ]\n",
      " [0.75922215]\n",
      " [0.92476356]\n",
      " [0.9753341 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 8400, cost: 0.1718427687883377\n",
      "hypothesis: [[0.04066882]\n",
      " [0.17045105]\n",
      " [0.34931606]\n",
      " [0.7617464 ]\n",
      " [0.9265344 ]\n",
      " [0.9758966 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 8600, cost: 0.1689244508743286\n",
      "hypothesis: [[0.03931564]\n",
      " [0.16905946]\n",
      " [0.34357542]\n",
      " [0.76421297]\n",
      " [0.9282444 ]\n",
      " [0.9764441 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 8800, cost: 0.16610267758369446\n",
      "hypothesis: [[0.03802261]\n",
      " [0.16767976]\n",
      " [0.33800566]\n",
      " [0.76662445]\n",
      " [0.92989665]\n",
      " [0.97697693]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 9000, cost: 0.16337305307388306\n",
      "hypothesis: [[0.03678662]\n",
      " [0.1663126 ]\n",
      " [0.33260038]\n",
      " [0.7689827 ]\n",
      " [0.93149346]\n",
      " [0.9774954 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 9200, cost: 0.16073136031627655\n",
      "hypothesis: [[0.03560451]\n",
      " [0.16495857]\n",
      " [0.32735336]\n",
      " [0.7712897 ]\n",
      " [0.9330371 ]\n",
      " [0.97799975]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 9400, cost: 0.15817348659038544\n",
      "hypothesis: [[0.03447345]\n",
      " [0.16361782]\n",
      " [0.32225776]\n",
      " [0.77354646]\n",
      " [0.9345296 ]\n",
      " [0.9784904 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 9600, cost: 0.15569566190242767\n",
      "hypothesis: [[0.03339094]\n",
      " [0.16229081]\n",
      " [0.31730837]\n",
      " [0.77575517]\n",
      " [0.93597335]\n",
      " [0.9789677 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 9800, cost: 0.15329430997371674\n",
      "hypothesis: [[0.03235424]\n",
      " [0.16097802]\n",
      " [0.3124999 ]\n",
      " [0.7779175 ]\n",
      " [0.9373705 ]\n",
      " [0.9794319 ]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n",
      "step: 10000, cost: 0.15096601843833923\n",
      "hypothesis: [[0.03136113]\n",
      " [0.15967917]\n",
      " [0.3078256 ]\n",
      " [0.7800342 ]\n",
      " [0.9387225 ]\n",
      " [0.97988325]], predicted: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]], accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#multi-variable Logistic Classification implement\n",
    "import tensorflow as tf\n",
    "#data\n",
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "#node\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "#weight, bias\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "#hypothesis\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "#cost function\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "#train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(cost)\n",
    "\n",
    "#Accuracy Computation\n",
    "#True if hypothesis > 0.5 else False\n",
    "predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "#launch\n",
    "with tf.Session() as sess:\n",
    "    #initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        h, p, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%200==0:\n",
    "            print('step: {0}, cost: {1}'.format(step, cost_val))\n",
    "            print('hypothesis: {0}, predicted: {1}, accuracy: {2}'.format(h, p, a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
