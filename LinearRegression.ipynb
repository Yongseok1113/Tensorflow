{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLinear Regression\\nSupervised Learning 을 통한 예측\\n1차원 그래프\\n\\n그래프상에서 실제 데이터들을 점들로 표현했을 때 \\n가설로 세운 1차 그래프에서 가까울 수록 가설이 좋음.\\n\\nCost(Loss) function\\n실제 데이터와 세운 가설이 얼마나 다른지 나타내는 그래프\\nH(x) - y -> 차이가 음수가 되거나 양수가 될 수 있기 때문에 좋지 않음\\n(H(x) -y)제곱이 좋음. 항상 양수로 표현되며, 값이 커질 수록(예측이 빗나간 정도가 클 수록) 더 강조 되기 때문\\n\\nH(x) = Wx + b일 때\\ncost(W,b) = 1/m * 시그마(i=1 ~ m까지) (H(x(i)) - y(i))제곱\\n\\ncost가 가장 작아지는 W,b값을 구하는 것이 Linear Regression의 목적\\n\\n원리\\n2차함수는 특정 값을 향해 수렴할 수 있음 현재 점에서 기울기를 통해 기울기가 작아지는 방향으로 나아가다 보면\\n결국 기울기가 0인 지점에 도달하게 된다. \\n\\n3차원에서 어느 점에서 시작하든 답을 향해 나아갈 수 있도록 cost함수를 설계해야 한다. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Linear Regression\n",
    "Supervised Learning 을 통한 예측\n",
    "1차원 그래프\n",
    "\n",
    "그래프상에서 실제 데이터들을 점들로 표현했을 때 \n",
    "가설로 세운 1차 그래프에서 가까울 수록 가설이 좋음.\n",
    "\n",
    "Cost(Loss) function\n",
    "실제 데이터와 세운 가설이 얼마나 다른지 나타내는 그래프\n",
    "H(x) - y -> 차이가 음수가 되거나 양수가 될 수 있기 때문에 좋지 않음\n",
    "(H(x) -y)제곱이 좋음. 항상 양수로 표현되며, 값이 커질 수록(예측이 빗나간 정도가 클 수록) 더 강조 되기 때문\n",
    "\n",
    "H(x) = Wx + b일 때\n",
    "cost(W,b) = 1/m * 시그마(i=1 ~ m까지) (H(x(i)) - y(i))제곱\n",
    "\n",
    "cost가 가장 작아지는 W,b값을 구하는 것이 Linear Regression의 목적\n",
    "\n",
    "원리\n",
    "2차함수는 특정 값을 향해 수렴할 수 있음 현재 점에서 기울기를 통해 기울기가 작아지는 방향으로 나아가다 보면\n",
    "결국 기울기가 0인 지점에 도달하게 된다. \n",
    "\n",
    "3차원에서 어느 점에서 시작하든 답을 향해 나아갈 수 있도록 cost함수를 설계해야 한다. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "0 6.902448 [-0.33296877] [0.27472213]\n",
      "20 0.12972552 [0.60083145] [0.64503396]\n",
      "40 0.06215657 [0.70254177] [0.65120983]\n",
      "60 0.055946887 [0.7244197] [0.6240798]\n",
      "80 0.050807282 [0.7381231] [0.59508115]\n",
      "100 0.046143923 [0.75050193] [0.56714606]\n",
      "120 0.041908655 [0.7622343] [0.5404952]\n",
      "140 0.038062103 [0.77340907] [0.5150942]\n",
      "160 0.034568653 [0.784058] [0.49088678]\n",
      "180 0.031395804 [0.7942065] [0.46781692]\n",
      "200 0.028514156 [0.80387807] [0.44583124]\n",
      "220 0.025897032 [0.81309503] [0.4248789]\n",
      "240 0.023520092 [0.82187885] [0.4049112]\n",
      "260 0.021361316 [0.8302499] [0.3858819]\n",
      "280 0.019400714 [0.8382275] [0.36774692]\n",
      "300 0.017620025 [0.8458302] [0.35046414]\n",
      "320 0.016002787 [0.8530756] [0.33399367]\n",
      "340 0.014533992 [0.8599806] [0.3182971]\n",
      "360 0.013199974 [0.86656106] [0.30333823]\n",
      "380 0.011988443 [0.8728322] [0.28908244]\n",
      "400 0.010888091 [0.87880856] [0.27549666]\n",
      "420 0.00988874 [0.8845041] [0.2625493]\n",
      "440 0.00898111 [0.889932] [0.2502105]\n",
      "460 0.0081568025 [0.8951048] [0.23845153]\n",
      "480 0.0074081384 [0.9000344] [0.2272452]\n",
      "500 0.0067281816 [0.90473247] [0.21656553]\n",
      "520 0.0061106444 [0.90920967] [0.2063877]\n",
      "540 0.0055497834 [0.91347647] [0.19668823]\n",
      "560 0.0050404114 [0.9175428] [0.18744466]\n",
      "580 0.004577776 [0.921418] [0.17863542]\n",
      "600 0.0041576074 [0.92511106] [0.17024021]\n",
      "620 0.0037760132 [0.9286304] [0.16223958]\n",
      "640 0.0034294396 [0.9319846] [0.15461494]\n",
      "660 0.0031146666 [0.93518114] [0.14734864]\n",
      "680 0.002828787 [0.9382273] [0.1404238]\n",
      "700 0.0025691537 [0.94113046] [0.13382441]\n",
      "720 0.0023333475 [0.9438971] [0.12753512]\n",
      "740 0.0021191773 [0.94653374] [0.1215414]\n",
      "760 0.0019246703 [0.94904643] [0.11582942]\n",
      "780 0.0017480152 [0.9514411] [0.11038586]\n",
      "800 0.0015875781 [0.95372313] [0.10519813]\n",
      "820 0.0014418656 [0.955898] [0.10025421]\n",
      "840 0.0013095248 [0.95797056] [0.09554264]\n",
      "860 0.001189331 [0.95994586] [0.09105248]\n",
      "880 0.0010801668 [0.9618283] [0.08677332]\n",
      "900 0.0009810239 [0.9636222] [0.0826953]\n",
      "920 0.0008909832 [0.96533185] [0.0788089]\n",
      "940 0.00080920506 [0.9669611] [0.07510518]\n",
      "960 0.0007349332 [0.9685138] [0.07157553]\n",
      "980 0.000667474 [0.9699937] [0.06821168]\n",
      "1000 0.0006062133 [0.9714038] [0.06500593]\n",
      "1020 0.0005505715 [0.97274774] [0.06195087]\n",
      "1040 0.00050003914 [0.97402847] [0.05903939]\n",
      "1060 0.00045414348 [0.97524905] [0.05626475]\n",
      "1080 0.00041245765 [0.9764123] [0.05362049]\n",
      "1100 0.00037460277 [0.9775208] [0.05110051]\n",
      "1120 0.00034021967 [0.9785772] [0.04869898]\n",
      "1140 0.00030899254 [0.9795839] [0.04641031]\n",
      "1160 0.00028063188 [0.98054355] [0.04422917]\n",
      "1180 0.00025487345 [0.98145795] [0.04215053]\n",
      "1200 0.00023148092 [0.98232937] [0.04016958]\n",
      "1220 0.00021023511 [0.9831598] [0.03828177]\n",
      "1240 0.00019093796 [0.9839512] [0.03648267]\n",
      "1260 0.0001734147 [0.9847054] [0.03476812]\n",
      "1280 0.00015749653 [0.9854242] [0.03313417]\n",
      "1300 0.00014304153 [0.9861092] [0.03157701]\n",
      "1320 0.00012991198 [0.98676205] [0.03009301]\n",
      "1340 0.00011798852 [0.98738414] [0.02867876]\n",
      "1360 0.000107159896 [0.987977] [0.02733097]\n",
      "1380 9.732374e-05 [0.9885421] [0.02604652]\n",
      "1400 8.839068e-05 [0.98908055] [0.02482244]\n",
      "1420 8.027825e-05 [0.98959374] [0.02365593]\n",
      "1440 7.2909526e-05 [0.990083] [0.0225441]\n",
      "1460 6.621715e-05 [0.99054897] [0.02148454]\n",
      "1480 6.0138376e-05 [0.99099314] [0.02047482]\n",
      "1500 5.4618966e-05 [0.99141645] [0.01951254]\n",
      "1520 4.9606755e-05 [0.9918198] [0.0185955]\n",
      "1540 4.5052573e-05 [0.99220425] [0.01772158]\n",
      "1560 4.0918272e-05 [0.9925706] [0.01688874]\n",
      "1580 3.716217e-05 [0.99291974] [0.01609505]\n",
      "1600 3.3751556e-05 [0.9932525] [0.01533865]\n",
      "1620 3.065396e-05 [0.9935696] [0.01461778]\n",
      "1640 2.7839798e-05 [0.99387187] [0.01393079]\n",
      "1660 2.528452e-05 [0.9941598] [0.01327607]\n",
      "1680 2.2963815e-05 [0.9944343] [0.01265216]\n",
      "1700 2.0856858e-05 [0.99469584] [0.01205754]\n",
      "1720 1.8942106e-05 [0.99494505] [0.01149095]\n",
      "1740 1.7203738e-05 [0.9951827] [0.01095094]\n",
      "1760 1.56249e-05 [0.9954091] [0.0104363]\n",
      "1780 1.4190246e-05 [0.99562484] [0.00994581]\n",
      "1800 1.288834e-05 [0.9958305] [0.00947838]\n",
      "1820 1.170499e-05 [0.99602646] [0.0090329]\n",
      "1840 1.0631032e-05 [0.99621314] [0.00860838]\n",
      "1860 9.655002e-06 [0.9963911] [0.00820384]\n",
      "1880 8.768901e-06 [0.9965607] [0.00781829]\n",
      "1900 7.963766e-06 [0.99672234] [0.00745088]\n",
      "1920 7.2331027e-06 [0.9968764] [0.00710071]\n",
      "1940 6.5689583e-06 [0.99702317] [0.00676701]\n",
      "1960 5.9662743e-06 [0.99716306] [0.00644899]\n",
      "1980 5.418457e-06 [0.9972964] [0.00614593]\n",
      "2000 4.9214873e-06 [0.9974234] [0.0058571]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "#[1] 은 [](Rank 1) 요소 1개(shape 1), rando_normal은 랜덤 메서드, reduce_mean 메서드는 평균 내주는 함수. 1/m 시그마 (i=1~m) 까지 의미\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = W * x_train + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "#minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "#launch the graph in session\n",
    "sess = tf.Session()\n",
    "#Initializes global variables in the graph\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#fit the line\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step%20 == 0 :\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.9214873e-06 [0.9974296] [0.00584302]\n",
      "20 4.469685e-06 [0.9975504] [0.00556842]\n",
      "40 4.0592895e-06 [0.9976655] [0.00530675]\n",
      "60 3.6870654e-06 [0.9977752] [0.00505738]\n",
      "80 3.3487222e-06 [0.9978798] [0.00481972]\n",
      "100 3.0411604e-06 [0.9979794] [0.00459321]\n",
      "120 2.7622307e-06 [0.99807435] [0.00437737]\n",
      "140 2.5085585e-06 [0.9981649] [0.00417165]\n",
      "160 2.2783117e-06 [0.99825114] [0.0039756]\n",
      "180 2.0692273e-06 [0.99833333] [0.00378876]\n",
      "200 1.8792585e-06 [0.9984116] [0.00361073]\n",
      "220 1.706846e-06 [0.9984863] [0.00344106]\n",
      "240 1.5501791e-06 [0.9985574] [0.00327933]\n",
      "260 1.4078955e-06 [0.9986252] [0.00312524]\n",
      "280 1.2787159e-06 [0.9986898] [0.00297838]\n",
      "300 1.1612435e-06 [0.99875134] [0.00283843]\n",
      "320 1.054888e-06 [0.99881] [0.00270505]\n",
      "340 9.579064e-07 [0.99886596] [0.00257794]\n",
      "360 8.7011927e-07 [0.99891925] [0.0024568]\n",
      "380 7.9015723e-07 [0.9989701] [0.00234134]\n",
      "400 7.17741e-07 [0.99901843] [0.00223132]\n",
      "420 6.5185026e-07 [0.99906456] [0.00212646]\n",
      "440 5.9204666e-07 [0.9991085] [0.00202652]\n",
      "460 5.376648e-07 [0.9991504] [0.0019313]\n",
      "480 4.883623e-07 [0.9991903] [0.00184058]\n",
      "500 4.4358535e-07 [0.9992283] [0.0017541]\n",
      "520 4.0286582e-07 [0.9992646] [0.0016717]\n",
      "540 3.65881e-07 [0.9992991] [0.00159317]\n",
      "560 3.3231947e-07 [0.999332] [0.00151834]\n",
      "580 3.0188275e-07 [0.9993634] [0.00144702]\n",
      "600 2.7416303e-07 [0.9993934] [0.00137904]\n",
      "620 2.489491e-07 [0.99942183] [0.00131425]\n",
      "640 2.2616281e-07 [0.99944896] [0.00125252]\n",
      "660 2.054199e-07 [0.9994748] [0.00119369]\n",
      "680 1.8660872e-07 [0.9994995] [0.00113762]\n",
      "700 1.6946684e-07 [0.99952304] [0.00108418]\n",
      "720 1.5393336e-07 [0.9995454] [0.00103324]\n",
      "740 1.3981095e-07 [0.99956685] [0.00098477]\n",
      "760 1.2697721e-07 [0.9995871] [0.00093854]\n",
      "780 1.1531343e-07 [0.99960643] [0.00089446]\n",
      "800 1.04754044e-07 [0.9996249] [0.00085245]\n",
      "820 9.517231e-08 [0.9996426] [0.00081242]\n",
      "840 8.64394e-08 [0.9996593] [0.00077432]\n",
      "860 7.847427e-08 [0.9996752] [0.00073796]\n",
      "880 7.1314865e-08 [0.9996906] [0.00070335]\n",
      "900 6.481796e-08 [0.99970496] [0.00067037]\n",
      "920 5.8838623e-08 [0.9997189] [0.0006389]\n",
      "940 5.3462585e-08 [0.999732] [0.00060897]\n",
      "960 4.8565735e-08 [0.9997446] [0.00058037]\n",
      "980 4.4101267e-08 [0.9997565] [0.00055322]\n",
      "1000 4.007005e-08 [0.999768] [0.00052723]\n",
      "1020 3.6414406e-08 [0.9997788] [0.00050254]\n",
      "1040 3.3052498e-08 [0.9997893] [0.00047893]\n",
      "1060 3.003934e-08 [0.999799] [0.00045649]\n",
      "1080 2.728911e-08 [0.99980855] [0.00043514]\n",
      "1100 2.4795048e-08 [0.9998175] [0.00041469]\n",
      "1120 2.2513651e-08 [0.99982595] [0.0003953]\n",
      "1140 2.0466913e-08 [0.9998343] [0.00037679]\n",
      "1160 1.8586286e-08 [0.999842] [0.00035906]\n",
      "1180 1.6898758e-08 [0.99984926] [0.00034228]\n",
      "1200 1.533713e-08 [0.9998564] [0.0003263]\n",
      "1220 1.3944245e-08 [0.99986327] [0.00031094]\n",
      "1240 1.2670536e-08 [0.9998695] [0.00029636]\n",
      "1260 1.1508239e-08 [0.99987555] [0.00028255]\n",
      "1280 1.0463377e-08 [0.9998815] [0.00026936]\n",
      "1300 9.499392e-09 [0.99988717] [0.00025665]\n",
      "1320 8.625595e-09 [0.9998923] [0.0002446]\n",
      "1340 7.847504e-09 [0.99989724] [0.00023319]\n",
      "1360 7.1329205e-09 [0.999902] [0.00022238]\n",
      "1380 6.4834516e-09 [0.9999068] [0.00021199]\n",
      "1400 5.8856813e-09 [0.9999112] [0.00020197]\n",
      "1420 5.346166e-09 [0.9999153] [0.00019248]\n",
      "1440 4.8596243e-09 [0.9999192] [0.0001835]\n",
      "1460 4.417378e-09 [0.9999228] [0.000175]\n",
      "1480 4.0173376e-09 [0.9999264] [0.00016694]\n",
      "1500 3.6545629e-09 [0.99992996] [0.00015917]\n",
      "1520 3.3100729e-09 [0.99993336] [0.00015163]\n",
      "1540 3.010906e-09 [0.9999365] [0.00014445]\n",
      "1560 2.727764e-09 [0.9999394] [0.00013766]\n",
      "1580 2.485308e-09 [0.9999422] [0.00013124]\n",
      "1600 2.2581996e-09 [0.9999448] [0.00012517]\n",
      "1620 2.0601998e-09 [0.99994725] [0.00011942]\n",
      "1640 1.8763682e-09 [0.99994963] [0.00011399]\n",
      "1660 1.707202e-09 [0.999952] [0.00010878]\n",
      "1680 1.5520195e-09 [0.9999544] [0.00010371]\n",
      "1700 1.403104e-09 [0.9999567] [9.8755445e-05]\n",
      "1720 1.2735389e-09 [0.99995875] [9.4016475e-05]\n",
      "1740 1.1565172e-09 [0.9999607] [8.954135e-05]\n",
      "1760 1.0467905e-09 [0.99996245] [8.529988e-05]\n",
      "1780 9.559784e-10 [0.99996424] [8.129524e-05]\n",
      "1800 8.648395e-10 [0.99996585] [7.75028e-05]\n",
      "1820 7.8821455e-10 [0.9999674] [7.3920164e-05]\n",
      "1840 7.168192e-10 [0.9999688] [7.0525086e-05]\n",
      "1860 6.5752676e-10 [0.9999702] [6.7325505e-05]\n",
      "1880 5.969696e-10 [0.99997157] [6.429123e-05]\n",
      "1900 5.4625104e-10 [0.9999728] [6.1419865e-05]\n",
      "1920 4.974036e-10 [0.999974] [5.8668513e-05]\n",
      "1940 4.538047e-10 [0.9999752] [5.604353e-05]\n",
      "1960 4.1221426e-10 [0.9999764] [5.3494834e-05]\n",
      "1980 3.7586764e-10 [0.9999776] [5.1006537e-05]\n",
      "2000 3.4100367e-10 [0.9999788] [4.854129e-05]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#같은 결과\n",
    "X = tf.placeholder(tf.float32, shape=[None])\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, W_val, b_val, _ = \\\n",
    "    sess.run([cost, W, b, train], feed_dict = {X:[1, 2, 3, 4, 5], Y:[2.1, 3.1, 4.1, 5.1, 6.1]})\n",
    "    if step%20 == 0: \n",
    "        print(step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = [1,2,3]\n",
    "Y = [1,2,3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "hypothesis = W * X\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "W_val = []\n",
    "cost_val = []\n",
    "\n",
    "for i in range(-30, 50):\n",
    "    feed_W = i * 0.1\n",
    "    curr_cost, curr_W = sess.run([cost, W], feed_dict={W: feed_W})\n",
    "    W_val.append(curr_W)\n",
    "    cost_val.append(curr_cost)\n",
    "    \n",
    "plt.plot(W_val, cost_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 13.742805480957031, W: [-0.7160673]\n",
      "step: 1, cost: 3.909064531326294, W: [0.08476412]\n",
      "step: 2, cost: 1.111911654472351, W: [0.5118742]\n",
      "step: 3, cost: 0.3162771463394165, W: [0.7396662]\n",
      "step: 4, cost: 0.08996324986219406, W: [0.86115533]\n",
      "step: 5, cost: 0.025589555501937866, W: [0.9259495]\n",
      "step: 6, cost: 0.007278793025761843, W: [0.96050644]\n",
      "step: 7, cost: 0.002070402493700385, W: [0.9789368]\n",
      "step: 8, cost: 0.000588918337598443, W: [0.98876625]\n",
      "step: 9, cost: 0.0001675139501458034, W: [0.99400866]\n",
      "step: 10, cost: 4.765029007103294e-05, W: [0.9968046]\n",
      "step: 11, cost: 1.3553642929764464e-05, W: [0.9982958]\n",
      "step: 12, cost: 3.8553330341528635e-06, W: [0.9990911]\n",
      "step: 13, cost: 1.096709070225188e-06, W: [0.99951524]\n",
      "step: 14, cost: 3.1205553341351333e-07, W: [0.99974144]\n",
      "step: 15, cost: 8.871533196952441e-08, W: [0.99986213]\n",
      "step: 16, cost: 2.5263815928155964e-08, W: [0.99992645]\n",
      "step: 17, cost: 7.187608730419015e-09, W: [0.9999608]\n",
      "step: 18, cost: 2.040097202282709e-09, W: [0.9999791]\n",
      "step: 19, cost: 5.784350776139036e-10, W: [0.99998885]\n",
      "step: 20, cost: 1.657933096366193e-10, W: [0.99999404]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#데이터 X일 때 Y의 결과가 나온다고 할 때 (=트레이닝 데이터가 주어질 때)\n",
    "X_data = [1, 2, 3]\n",
    "Y_data = [1, 2, 3]\n",
    "\n",
    "#계수 W는 현재 알 수 없는 값\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "#가설 함수 hypothesis\n",
    "hypothesis = W * X_data\n",
    "\n",
    "#목적 함수 cost cost(W) = 실제 데이터 들이 가설함수와 떨어진 거리의 제곱값 들의 평균치\n",
    "#cost(W)가 작다는 것은 평균적으로 실제 데이터들이 가설함수에 근접했다는 의미가 된다. 즉 평균치의 최솟값을 구하는 것이 목적\n",
    "#cost = 1/m 시그마(i=1~m) ( H(i번째 x) - (i번째 y) )제곱\n",
    "#tf.reduce_mean(수식) 메서드 = 1/m 시그마(i=1~m) = 시그마 합의 평균을 반환하는 메서드\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "'''\n",
    "gradient descent 라는 것은\n",
    "\n",
    "어떤 한점에서 기울기를 구했을 때 기울기가 작아지는 방향으로 나아가겠다는 의미이다. 따라서 W 현재값에 미분한 식(기울기)를 뺀다. \n",
    "\n",
    "식으로 표현하면\n",
    "\n",
    "    W := W - (알파)* 1/m 시그마(i=1~m) { (W * x(i) - y(i))* x(i) }(=미분한 식)\n",
    "    \n",
    "만약 기울기를 더한다면 커지는 방향으로 나아가겠다는 의미\n",
    "알파 = Learning rate\n",
    "'''\n",
    "\n",
    "#minimize : gradient descent using derivative : W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W*X - Y) * X)\n",
    "descent = W - learning_rate * gradient\n",
    "update = W.assign(descent) \n",
    "\n",
    "'''\n",
    "#minimize : gradient descent magic\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "#세션 준비\n",
    "sess = tf.Session()\n",
    "#세션 변수 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#launch\n",
    "for step in range(100):\n",
    "    print(step, sess.run(W))\n",
    "    sess.run(train)\n",
    "\n",
    "'''\n",
    "#실행\n",
    "\n",
    "#세션 준비\n",
    "sess = tf.Session()\n",
    "#세션 변수 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(21) :\n",
    "    sess.run(update, feed_dict={X: X_data, Y: Y_data})\n",
    "    print('step: {0}, cost: {1}, W: {2}'.format(step, sess.run(cost, feed_dict={X: X_data, Y: Y_data}), sess.run(W)) )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [37.333332, 5.0, [(37.333336, 5.0)]]\n",
      "1 [33.84889, 4.6266665, [(33.84889, 4.6266665)]]\n",
      "2 [30.689657, 4.2881775, [(30.689657, 4.2881775)]]\n",
      "3 [27.825287, 3.9812808, [(27.825287, 3.9812808)]]\n",
      "4 [25.228262, 3.703028, [(25.228262, 3.703028)]]\n",
      "5 [22.873621, 3.4507453, [(22.873623, 3.4507453)]]\n",
      "6 [20.738752, 3.2220092, [(20.73875, 3.2220092)]]\n",
      "7 [18.803137, 3.0146217, [(18.803137, 3.0146217)]]\n",
      "8 [17.048176, 2.8265903, [(17.048176, 2.8265903)]]\n",
      "9 [15.457013, 2.6561086, [(15.457014, 2.6561086)]]\n",
      "10 [14.014359, 2.5015385, [(14.01436, 2.5015385)]]\n",
      "11 [12.706352, 2.361395, [(12.706352, 2.361395)]]\n",
      "12 [11.520427, 2.2343314, [(11.520427, 2.2343314)]]\n",
      "13 [10.445186, 2.119127, [(10.445185, 2.119127)]]\n",
      "14 [9.470302, 2.0146751, [(9.470302, 2.0146751)]]\n",
      "15 [8.586407, 1.9199722, [(8.586407, 1.9199722)]]\n",
      "16 [7.785009, 1.8341081, [(7.785009, 1.8341081)]]\n",
      "17 [7.0584083, 1.756258, [(7.0584083, 1.756258)]]\n",
      "18 [6.399624, 1.685674, [(6.399624, 1.685674)]]\n",
      "19 [5.8023257, 1.6216778, [(5.8023252, 1.6216778)]]\n",
      "20 [5.260776, 1.5636545, [(5.260776, 1.5636545)]]\n",
      "21 [4.7697697, 1.5110468, [(4.7697697, 1.5110468)]]\n",
      "22 [4.324591, 1.4633491, [(4.324591, 1.4633491)]]\n",
      "23 [3.9209633, 1.4201032, [(3.9209635, 1.4201032)]]\n",
      "24 [3.5550067, 1.3808936, [(3.5550067, 1.3808936)]]\n",
      "25 [3.2232056, 1.3453435, [(3.2232056, 1.3453435)]]\n",
      "26 [2.9223735, 1.3131114, [(2.9223735, 1.3131114)]]\n",
      "27 [2.6496189, 1.2838877, [(2.6496186, 1.2838877)]]\n",
      "28 [2.4023216, 1.2573916, [(2.4023216, 1.2573916)]]\n",
      "29 [2.178105, 1.2333684, [(2.178105, 1.2333684)]]\n",
      "30 [1.9748148, 1.2115873, [(1.9748147, 1.2115873)]]\n",
      "31 [1.7904993, 1.1918392, [(1.7904994, 1.1918392)]]\n",
      "32 [1.623386, 1.1739342, [(1.6233861, 1.1739342)]]\n",
      "33 [1.4718695, 1.1577003, [(1.4718695, 1.1577003)]]\n",
      "34 [1.3344955, 1.1429816, [(1.3344957, 1.1429816)]]\n",
      "35 [1.2099417, 1.1296366, [(1.2099419, 1.1296366)]]\n",
      "36 [1.0970144, 1.1175373, [(1.0970144, 1.1175373)]]\n",
      "37 [0.9946267, 1.1065671, [(0.9946267, 1.1065671)]]\n",
      "38 [0.90179497, 1.0966209, [(0.901795, 1.0966209)]]\n",
      "39 [0.8176275, 1.087603, [(0.81762755, 1.087603)]]\n",
      "40 [0.7413151, 1.0794266, [(0.7413151, 1.0794266)]]\n",
      "41 [0.67212623, 1.0720135, [(0.67212623, 1.0720135)]]\n",
      "42 [0.609394, 1.0652922, [(0.609394, 1.0652922)]]\n",
      "43 [0.5525169, 1.0591983, [(0.5525169, 1.0591983)]]\n",
      "44 [0.50094914, 1.0536731, [(0.50094914, 1.0536731)]]\n",
      "45 [0.45419374, 1.0486636, [(0.45419377, 1.0486636)]]\n",
      "46 [0.41180158, 1.0441216, [(0.41180158, 1.0441216)]]\n",
      "47 [0.37336722, 1.0400037, [(0.37336725, 1.0400037)]]\n",
      "48 [0.33851996, 1.03627, [(0.33852, 1.03627)]]\n",
      "49 [0.30692515, 1.0328848, [(0.30692515, 1.0328848)]]\n",
      "50 [0.27827826, 1.0298156, [(0.2782783, 1.0298156)]]\n",
      "51 [0.25230527, 1.0270327, [(0.25230527, 1.0270327)]]\n",
      "52 [0.2287569, 1.0245097, [(0.2287569, 1.0245097)]]\n",
      "53 [0.20740573, 1.022222, [(0.20740573, 1.022222)]]\n",
      "54 [0.18804836, 1.020148, [(0.18804836, 1.020148)]]\n",
      "55 [0.17049654, 1.0182675, [(0.17049655, 1.0182675)]]\n",
      "56 [0.15458433, 1.0165626, [(0.15458433, 1.0165626)]]\n",
      "57 [0.14015675, 1.0150168, [(0.14015675, 1.0150168)]]\n",
      "58 [0.12707591, 1.0136153, [(0.12707591, 1.0136153)]]\n",
      "59 [0.11521538, 1.0123445, [(0.11521538, 1.0123445)]]\n",
      "60 [0.10446167, 1.0111923, [(0.10446167, 1.0111923)]]\n",
      "61 [0.09471202, 1.0101477, [(0.09471202, 1.0101477)]]\n",
      "62 [0.08587202, 1.0092006, [(0.08587202, 1.0092006)]]\n",
      "63 [0.07785805, 1.0083419, [(0.07785805, 1.0083419)]]\n",
      "64 [0.07059129, 1.0075634, [(0.07059129, 1.0075634)]]\n",
      "65 [0.06400236, 1.0068574, [(0.06400236, 1.0068574)]]\n",
      "66 [0.05802846, 1.0062174, [(0.05802846, 1.0062174)]]\n",
      "67 [0.052612226, 1.005637, [(0.052612226, 1.005637)]]\n",
      "68 [0.047702473, 1.005111, [(0.047702473, 1.005111)]]\n",
      "69 [0.043249767, 1.0046339, [(0.043249767, 1.0046339)]]\n",
      "70 [0.03921318, 1.0042014, [(0.03921318, 1.0042014)]]\n",
      "71 [0.035553534, 1.0038093, [(0.035553537, 1.0038093)]]\n",
      "72 [0.032236177, 1.0034539, [(0.03223618, 1.0034539)]]\n",
      "73 [0.029227654, 1.0031315, [(0.029227655, 1.0031315)]]\n",
      "74 [0.02649951, 1.0028392, [(0.02649951, 1.0028392)]]\n",
      "75 [0.024025917, 1.0025742, [(0.024025917, 1.0025742)]]\n",
      "76 [0.021783749, 1.002334, [(0.02178375, 1.002334)]]\n",
      "77 [0.01975123, 1.0021162, [(0.019751232, 1.0021162)]]\n",
      "78 [0.017907381, 1.0019187, [(0.017907381, 1.0019187)]]\n",
      "79 [0.016236702, 1.0017396, [(0.016236704, 1.0017396)]]\n",
      "80 [0.014720838, 1.0015773, [(0.014720838, 1.0015773)]]\n",
      "81 [0.01334699, 1.00143, [(0.013346991, 1.00143)]]\n",
      "82 [0.012100856, 1.0012965, [(0.012100856, 1.0012965)]]\n",
      "83 [0.010971785, 1.0011755, [(0.010971785, 1.0011755)]]\n",
      "84 [0.0099481745, 1.0010659, [(0.0099481745, 1.0010659)]]\n",
      "85 [0.009018898, 1.0009663, [(0.009018898, 1.0009663)]]\n",
      "86 [0.008176883, 1.0008761, [(0.008176884, 1.0008761)]]\n",
      "87 [0.007413149, 1.0007943, [(0.007413149, 1.0007943)]]\n",
      "88 [0.006721576, 1.0007201, [(0.006721576, 1.0007201)]]\n",
      "89 [0.0060940585, 1.0006529, [(0.0060940585, 1.0006529)]]\n",
      "90 [0.005525271, 1.000592, [(0.0055252714, 1.000592)]]\n",
      "91 [0.0050098896, 1.0005368, [(0.0050098896, 1.0005368)]]\n",
      "92 [0.004542589, 1.0004867, [(0.004542589, 1.0004867)]]\n",
      "93 [0.0041189194, 1.0004413, [(0.0041189194, 1.0004413)]]\n",
      "94 [0.0037339528, 1.0004001, [(0.003733953, 1.0004001)]]\n",
      "95 [0.0033854644, 1.0003628, [(0.0033854644, 1.0003628)]]\n",
      "96 [0.0030694802, 1.0003289, [(0.0030694804, 1.0003289)]]\n",
      "97 [0.0027837753, 1.0002983, [(0.0027837753, 1.0002983)]]\n",
      "98 [0.0025234222, 1.0002704, [(0.0025234222, 1.0002704)]]\n",
      "99 [0.0022875469, 1.0002451, [(0.0022875469, 1.0002451)]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "#tf.Variable(tf.random_normal([1]), name='weight')\n",
    "W = tf.Variable(5.)\n",
    "\n",
    "hypothesis = W * X\n",
    "#직접 계산한 기울기 값\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "'''\n",
    "#optimizer.minimize(cost)는 가능하지만 compute_gradients는 함수와 변수 둘 다 넣어줘야함\n",
    "\n",
    "# Create an optimizer.\n",
    "opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "\n",
    "# Compute the gradients for a list of variables.\n",
    "grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "\n",
    "# grads_and_vars is a list of tuples (gradient, variable).  Do whatever you\n",
    "# need to the 'gradient' part, for example cap them, etc.\n",
    "capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "\n",
    "# Ask the optimizer to apply the capped gradients.\n",
    "opt.apply_gradients(capped_grads_and_vars)\n",
    "'''\n",
    "#get gradient 기울기 추출 = tensorflow가 계산한 기울기 값\n",
    "gvs = optimizer.compute_gradients(cost, W)\n",
    "#apply gradients 기울기 적용\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "\n",
    "#최솟값\n",
    "#train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(100) :\n",
    "    print(step, sess.run([gradient, W, gvs]) )\n",
    "    sess.run(apply_gradients)\n",
    "#직접계산한 gradient 값과 tensorflow가 계산한 gradient 값이 동일함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
