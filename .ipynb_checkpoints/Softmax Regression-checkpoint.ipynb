{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSoftmax Regression?\\nMultinomial Classification : 세가지 이상으로 분류\\n\\nex) 점수에 따른 학점 분류\\n\\nbinary classifiaction 여러개로 구현한다.\\n\\nA or not classifiation\\nB or not ,,\\nC or not ,,\\n\\n[a1, a2, a3] X [[x1], [x2], [x3]] = Y1\\n[b1, b2, b3] X [[x1], [x2], [x3]] = Y2\\n[c1, c2, c3] X [[x1], [x2], [x3]] = Y3 \\n\\n=>\\n\\n[ [a1, a2, a3],\\n  [b1, b2, b3],\\n  [c1, c2, c3]] X [[x1], [x2], [x3]] => [[Y1], [Y2], [Y3]]\\n\\n결과값이 행렬로 표현 된다고 할 때, \\n\\n이것을 소프트맥스 함수를 이용하여 각 결과값을 0과1사이 수치값(확률?)으로 계산한다. \\n그 다음 가장 큰 값(가장 높은 확률)을 1로 나머지를 0으로 바꾼다.\\n\\n예를들어 [[Y1], [Y2], [Y3]] => [[0.7], [0.2], [0.1]] => [[1], [0], [0]] 이다\\n\\nhypothesis = tf.nn.softmax(tf.matmul(W, X))\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Softmax Regression?\n",
    "Multinomial Classification : 세가지 이상으로 분류\n",
    "\n",
    "ex) 점수에 따른 학점 분류\n",
    "\n",
    "binary classifiaction 여러개로 구현한다.\n",
    "\n",
    "A or not classifiation\n",
    "B or not ,,\n",
    "C or not ,,\n",
    "\n",
    "[a1, a2, a3] X [[x1], [x2], [x3]] = Y1\n",
    "[b1, b2, b3] X [[x1], [x2], [x3]] = Y2\n",
    "[c1, c2, c3] X [[x1], [x2], [x3]] = Y3 \n",
    "\n",
    "=>\n",
    "\n",
    "[ [a1, a2, a3],\n",
    "  [b1, b2, b3],\n",
    "  [c1, c2, c3]] X [[x1], [x2], [x3]] => [[Y1], [Y2], [Y3]]\n",
    "\n",
    "결과값이 행렬로 표현 된다고 할 때, \n",
    "\n",
    "이것을 소프트맥스 함수를 이용하여 각 결과값을 0과1사이 수치값(확률?)으로 계산한다. \n",
    "그 다음 가장 큰 값(가장 높은 확률)을 1로 나머지를 0으로 바꾼다.\n",
    "\n",
    "예를들어 [[Y1], [Y2], [Y3]] => [[0.7], [0.2], [0.1]] => [[1], [0], [0]] 이다\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(W, X))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "step: 0, hypothesis: [[9.86676335e-01 1.23126870e-02 1.01095403e-03]\n",
      " [9.72317934e-01 2.45906934e-02 3.09129152e-03]\n",
      " [7.02454209e-01 2.96048880e-01 1.49683410e-03]\n",
      " [8.52935433e-01 1.46693528e-01 3.70979396e-04]\n",
      " [9.99998212e-01 1.74975003e-06 1.76405609e-11]\n",
      " [9.84459043e-01 1.55306365e-02 1.02853146e-05]\n",
      " [9.99992967e-01 7.01054705e-06 9.22077553e-11]\n",
      " [9.99998808e-01 1.13884096e-06 2.08561141e-12]], cost: 4.815103530883789\n",
      "step: 200, hypothesis: [[0.5777815  0.22801605 0.19420242]\n",
      " [0.11374126 0.16045448 0.7258042 ]\n",
      " [0.00865372 0.5774269  0.41391933]\n",
      " [0.00272191 0.3663601  0.63091797]\n",
      " [0.80322915 0.19490938 0.00186138]\n",
      " [0.04464673 0.80504566 0.15030761]\n",
      " [0.5577285  0.43422976 0.00804171]\n",
      " [0.55515355 0.44160143 0.00324505]], cost: 0.8587527871131897\n",
      "step: 400, hypothesis: [[0.36023414 0.22637472 0.4133911 ]\n",
      " [0.09849668 0.15174888 0.7497544 ]\n",
      " [0.00877393 0.5739068  0.41731927]\n",
      " [0.00394601 0.44202277 0.55403125]\n",
      " [0.72944933 0.25833628 0.01221446]\n",
      " [0.10197809 0.77610123 0.12192068]\n",
      " [0.5800865  0.39904252 0.02087096]\n",
      " [0.5997838  0.39103338 0.0091828 ]], cost: 0.6905497312545776\n",
      "step: 600, hypothesis: [[0.2255183  0.19296154 0.58152014]\n",
      " [0.08970916 0.14747691 0.7628139 ]\n",
      " [0.00856389 0.5636193  0.42781684]\n",
      " [0.00486705 0.49459282 0.50054014]\n",
      " [0.6650693  0.305771   0.02915978]\n",
      " [0.16996118 0.7463372  0.08370169]\n",
      " [0.59192526 0.38102296 0.0270518 ]\n",
      " [0.62475103 0.3636499  0.01159906]], cost: 0.6047756671905518\n",
      "step: 800, hypothesis: [[0.15273592 0.16115709 0.68610704]\n",
      " [0.08026963 0.14526011 0.77447027]\n",
      " [0.0074984  0.54758495 0.44491664]\n",
      " [0.00485217 0.5272761  0.4678717 ]\n",
      " [0.623174   0.33576256 0.04106333]\n",
      " [0.21873263 0.72472656 0.05654078]\n",
      " [0.59885305 0.37542588 0.02572104]\n",
      " [0.63892657 0.3508335  0.01023996]], cost: 0.557026743888855\n",
      "step: 1000, hypothesis: [[0.1105489  0.13607559 0.75337553]\n",
      " [0.07084321 0.1440444  0.7851123 ]\n",
      " [0.00612562 0.52962863 0.46424574]\n",
      " [0.0042671  0.5495442  0.4461887 ]\n",
      " [0.5998636  0.35384327 0.04629321]\n",
      " [0.24518734 0.71492916 0.03988351]\n",
      " [0.6039227  0.3740696  0.02200764]\n",
      " [0.64882815 0.34313533 0.00803658]], cost: 0.525312066078186\n",
      "step: 1200, hypothesis: [[0.08362479 0.11670785 0.79966736]\n",
      " [0.06256606 0.14341871 0.7940152 ]\n",
      " [0.00487393 0.5118241  0.483302  ]\n",
      " [0.00356866 0.56662476 0.42980662]\n",
      " [0.58646774 0.36594108 0.04759115]\n",
      " [0.25831065 0.71216404 0.02952535]\n",
      " [0.6079899  0.37373108 0.01827902]\n",
      " [0.6568198  0.33705238 0.00612776]], cost: 0.5015069842338562\n",
      "step: 1400, hypothesis: [[0.06514189 0.10155606 0.833302  ]\n",
      " [0.05565923 0.1431638  0.80117697]\n",
      " [0.00385841 0.49497357 0.5011681 ]\n",
      " [0.00294281 0.5808585  0.4161987 ]\n",
      " [0.57761407 0.37534764 0.04703826]\n",
      " [0.26478124 0.71246016 0.02275857]\n",
      " [0.6113585  0.37349826 0.01514319]\n",
      " [0.66375434 0.3315632  0.00468239]], cost: 0.4823679029941559\n",
      "step: 1600, hypothesis: [[0.05180401 0.08945616 0.85873985]\n",
      " [0.0499463  0.14312989 0.8069238 ]\n",
      " [0.00306389 0.47930393 0.51763225]\n",
      " [0.00242685 0.5933066  0.4042665 ]\n",
      " [0.5707628  0.38352507 0.04571217]\n",
      " [0.26796854 0.71391463 0.01811684]\n",
      " [0.6142257  0.37313944 0.01263483]\n",
      " [0.67002946 0.32634732 0.00362321]], cost: 0.46631476283073425\n",
      "step: 1800, hypothesis: [[0.04183948 0.07959493 0.87856567]\n",
      " [0.04518383 0.1432091  0.81160706]\n",
      " [0.00244661 0.46481168 0.53274167]\n",
      " [0.00201111 0.60448706 0.3935018 ]\n",
      " [0.56480265 0.39108002 0.04411732]\n",
      " [0.26942953 0.7157733  0.01479723]\n",
      " [0.61674285 0.3726076  0.0106496 ]\n",
      " [0.6758686  0.32128453 0.00284693]], cost: 0.4524489939212799\n",
      "step: 2000, hypothesis: [[0.03420507 0.07141285 0.89438206]\n",
      " [0.04116242 0.14332582 0.8155118 ]\n",
      " [0.00196546 0.45141405 0.5466205 ]\n",
      " [0.00167682 0.6146809  0.3836423 ]\n",
      " [0.55924904 0.39826983 0.04248116]\n",
      " [0.26989305 0.7177678  0.01233919]\n",
      " [0.61901814 0.37190974 0.00907212]\n",
      " [0.6814049  0.31632382 0.00227127]], cost: 0.4402084946632385\n",
      "step: 2200, hypothesis: [[0.02824256 0.06452003 0.90723735]\n",
      " [0.03772157 0.14342964 0.8188488 ]\n",
      " [0.00158801 0.43900868 0.55940336]\n",
      " [0.00140667 0.6240609  0.3745324 ]\n",
      " [0.55388844 0.40520987 0.04090166]\n",
      " [0.26972088 0.71981364 0.01046548]\n",
      " [0.62112665 0.37106678 0.00780656]\n",
      " [0.6867196  0.31144273 0.00183771]], cost: 0.42922019958496094\n",
      "step: 2400, hypothesis: [[0.02351423 0.05863811 0.91784763]\n",
      " [0.03474192 0.14348796 0.82177013]\n",
      " [0.00128984 0.4274961  0.571214  ]\n",
      " [0.00118681 0.63274837 0.36606488]\n",
      " [0.5486241  0.41195747 0.03941839]\n",
      " [0.26911002 0.721888   0.00900193]\n",
      " [0.6231223  0.37009794 0.00677976]\n",
      " [0.69186586 0.30662826 0.0015059 ]], cost: 0.4192235469818115\n",
      "step: 2600, hypothesis: [[0.01971751 0.05356361 0.92671895]\n",
      " [0.03213454 0.14348084 0.8243846 ]\n",
      " [0.00105273 0.41678506 0.5821622 ]\n",
      " [0.00100658 0.64083284 0.35816061]\n",
      " [0.54341245 0.41854483 0.03804271]\n",
      " [0.26817822 0.7239869  0.00783482]\n",
      " [0.62504077 0.3690221  0.00593706]\n",
      " [0.6968783  0.30187368 0.00124802]], cost: 0.4100314974784851\n",
      "step: 2800, hypothesis: [[1.6636744e-02 4.9144607e-02 9.3421865e-01]\n",
      " [2.9832331e-02 1.4339744e-01 8.2677019e-01]\n",
      " [8.6301897e-04 4.0679449e-01 5.9234244e-01]\n",
      " [8.5781515e-04 6.4838701e-01 3.5075516e-01]\n",
      " [5.3823286e-01 4.2499346e-01 3.6773644e-02]\n",
      " [2.6700142e-01 7.2611117e-01 6.8874592e-03]\n",
      " [6.2690604e-01 3.6785609e-01 5.2378182e-03]\n",
      " [7.0177758e-01 2.9717767e-01 1.0447419e-03]], cost: 0.4015052914619446\n",
      "step: 3000, hypothesis: [[1.4114492e-02 4.5265153e-02 9.4062030e-01]\n",
      " [2.7784027e-02 1.4323275e-01 8.2898319e-01]\n",
      " [7.1039732e-04 3.9745310e-01 6.0183650e-01]\n",
      " [7.3425123e-04 6.5547073e-01 3.4379503e-01]\n",
      " [5.3307778e-01 4.3131760e-01 3.5604633e-02]\n",
      " [2.6563281e-01 7.2826058e-01 6.1065862e-03]\n",
      " [6.2873608e-01 3.6661226e-01 4.6516284e-03]\n",
      " [7.0657927e-01 2.9253832e-01 8.8240980e-04]], cost: 0.39353907108306885\n",
      "step: 3200, hypothesis: [[1.20334085e-02 4.18351702e-02 9.46131349e-01]\n",
      " [2.59496383e-02 1.42985776e-01 8.31064582e-01]\n",
      " [5.86994982e-04 3.88697922e-01 6.10715091e-01]\n",
      " [6.31030591e-04 6.62134230e-01 3.37234646e-01]\n",
      " [5.27945459e-01 4.37527299e-01 3.45272720e-02]\n",
      " [2.64112532e-01 7.30433106e-01 5.45432791e-03]\n",
      " [6.30542755e-01 3.65301669e-01 4.15559160e-03]\n",
      " [7.11293638e-01 2.87955135e-01 7.51252868e-04]], cost: 0.3860514163970947\n",
      "step: 3400, hypothesis: [[1.0304600e-02 3.8783640e-02 9.5091176e-01]\n",
      " [2.4297483e-02 1.4265849e-01 8.3304405e-01]\n",
      " [4.8675685e-04 3.8047379e-01 6.1903942e-01]\n",
      " [5.4435065e-04 6.6842109e-01 3.3103448e-01]\n",
      " [5.2283657e-01 4.4363081e-01 3.3532627e-02]\n",
      " [2.6246962e-01 7.3262721e-01 4.9031102e-03]\n",
      " [6.3233393e-01 3.6393386e-01 3.7322200e-03]\n",
      " [7.1592671e-01 2.8342912e-01 6.4415456e-04]], cost: 0.37897777557373047\n",
      "step: 3600, hypothesis: [[8.8597210e-03 3.6053684e-02 9.5508665e-01]\n",
      " [2.2802172e-02 1.4225437e-01 8.3494341e-01]\n",
      " [4.0499121e-04 3.7273088e-01 6.2686414e-01]\n",
      " [4.7121540e-04 6.7436820e-01 3.2516050e-01]\n",
      " [5.1775348e-01 4.4963425e-01 3.2612290e-02]\n",
      " [2.6072893e-01 7.3483860e-01 4.4324864e-03]\n",
      " [6.3411504e-01 3.6251691e-01 3.3680389e-03]\n",
      " [7.2048306e-01 2.7896106e-01 5.5586407e-04]], cost: 0.37226659059524536\n",
      "step: 3800, hypothesis: [[7.6455865e-03 3.3599313e-02 9.5875514e-01]\n",
      " [2.1442952e-02 1.4177811e-01 8.3677888e-01]\n",
      " [3.3803136e-04 3.6542588e-01 6.3423610e-01]\n",
      " [4.0923691e-04 6.8000853e-01 3.1958222e-01]\n",
      " [5.1269972e-01 4.5554212e-01 3.1758152e-02]\n",
      " [2.5890848e-01 7.3706454e-01 4.0269918e-03]\n",
      " [6.3588983e-01 3.6105764e-01 3.0525166e-03]\n",
      " [7.2496521e-01 2.7455238e-01 4.8244317e-04]], cost: 0.3658759593963623\n",
      "step: 4000, hypothesis: [[6.6203503e-03 3.1382859e-02 9.6199679e-01]\n",
      " [2.0202763e-02 1.4123492e-01 8.3856225e-01]\n",
      " [2.8299508e-04 3.5851958e-01 6.4119744e-01]\n",
      " [3.5650068e-04 6.8537033e-01 3.1427324e-01]\n",
      " [5.0767916e-01 4.6135750e-01 3.0963311e-02]\n",
      " [2.5702414e-01 7.3930109e-01 3.6747744e-03]\n",
      " [6.3766134e-01 3.5956126e-01 2.7773748e-03]\n",
      " [7.2937596e-01 2.7020314e-01 4.2090728e-04]], cost: 0.3597713112831116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4200, hypothesis: [[5.7507483e-03 2.9373126e-02 9.6487612e-01]\n",
      " [1.9067293e-02 1.4063007e-01 8.4030259e-01]\n",
      " [2.3760315e-04 3.5197717e-01 6.4778525e-01]\n",
      " [3.1145886e-04 6.9047767e-01 3.0921081e-01]\n",
      " [5.0269443e-01 4.6708417e-01 3.0221410e-02]\n",
      " [2.5508884e-01 7.4154454e-01 3.3666138e-03]\n",
      " [6.3943046e-01 3.5803351e-01 2.5360091e-03]\n",
      " [7.3371559e-01 2.6591545e-01 3.6896259e-04]], cost: 0.3539241552352905\n",
      "step: 4400, hypothesis: [[5.0101443e-03 2.7544234e-02 9.6744567e-01]\n",
      " [1.8024577e-02 1.3996945e-01 8.4200597e-01]\n",
      " [2.0004324e-04 3.4576756e-01 6.5403241e-01]\n",
      " [2.7285414e-04 6.9535357e-01 3.0437356e-01]\n",
      " [4.9774915e-01 4.7272408e-01 2.9526863e-02]\n",
      " [2.5311360e-01 7.4379110e-01 3.0952352e-03]\n",
      " [6.4119828e-01 3.5647854e-01 2.3231145e-03]\n",
      " [7.3798555e-01 2.6168963e-01 3.2482666e-04]], cost: 0.3483104407787323\n",
      "step: 4600, hypothesis: [[4.3770215e-03 2.5874278e-02 9.6974874e-01]\n",
      " [1.7064411e-02 1.3925849e-01 8.4367710e-01]\n",
      " [1.6886805e-04 3.3986294e-01 6.5996820e-01]\n",
      " [2.3965753e-04 7.0001709e-01 2.9974326e-01]\n",
      " [4.9284613e-01 4.7827908e-01 2.8874744e-02]\n",
      " [2.5110689e-01 7.4603826e-01 2.8548306e-03]\n",
      " [6.4296502e-01 3.5490057e-01 2.1343916e-03]\n",
      " [7.4218583e-01 2.5752711e-01 2.8710245e-04]], cost: 0.34290972352027893\n",
      "step: 4800, hypothesis: [[3.8338930e-03 2.4344783e-02 9.7182131e-01]\n",
      " [1.6178086e-02 1.3850236e-01 8.4531957e-01]\n",
      " [1.4291635e-04 3.3423832e-01 6.6561872e-01]\n",
      " [2.1102359e-04 7.0448524e-01 2.9530373e-01]\n",
      " [4.8798782e-01 4.8375130e-01 2.8260875e-02]\n",
      " [2.4907693e-01 7.4828237e-01 2.6407407e-03]\n",
      " [6.4473039e-01 3.5330328e-01 1.9663307e-03]\n",
      " [7.4631739e-01 2.5342792e-01 2.5468171e-04]], cost: 0.337704598903656\n",
      "step: 5000, hypothesis: [[3.36644240e-03 2.29399428e-02 9.73693609e-01]\n",
      " [1.53580345e-02 1.37706250e-01 8.46935630e-01]\n",
      " [1.21251760e-04 3.28871191e-01 6.71007574e-01]\n",
      " [1.86252393e-04 7.08773971e-01 2.91039795e-01]\n",
      " [4.83177871e-01 4.89140689e-01 2.76814476e-02]\n",
      " [2.47030303e-01 7.50520647e-01 2.44915462e-03]\n",
      " [6.46495163e-01 3.51688802e-01 1.81603467e-03]\n",
      " [7.50380814e-01 2.49392480e-01 2.26678996e-04]], cost: 0.33267998695373535\n",
      "step: 5200, hypothesis: [[2.9629034e-03 2.1646319e-02 9.7539073e-01]\n",
      " [1.4597695e-02 1.3687487e-01 8.4852737e-01]\n",
      " [1.0311643e-04 3.2374078e-01 6.7615610e-01]\n",
      " [1.6476244e-04 7.1289611e-01 2.8693911e-01]\n",
      " [4.7841674e-01 4.9444997e-01 2.7133223e-02]\n",
      " [2.4497254e-01 7.5275052e-01 2.2769617e-03]\n",
      " [6.4825815e-01 3.5006073e-01 1.6811047e-03]\n",
      " [7.5437558e-01 2.4542214e-01 2.0238101e-04]], cost: 0.32782286405563354\n",
      "step: 5400, hypothesis: [[2.6135473e-03 2.0452224e-02 9.7693425e-01]\n",
      " [1.3891371e-02 1.3601275e-01 8.5009593e-01]\n",
      " [8.7896624e-05 3.1883013e-01 6.8108195e-01]\n",
      " [1.4607029e-04 7.1686524e-01 2.8298867e-01]\n",
      " [4.7370806e-01 4.9967882e-01 2.6613176e-02]\n",
      " [2.4290788e-01 7.5497061e-01 2.1215624e-03]\n",
      " [6.5001965e-01 3.4842083e-01 1.5595248e-03]\n",
      " [7.5830239e-01 2.4151637e-01 1.8120620e-04]], cost: 0.32312172651290894\n",
      "step: 5600, hypothesis: [[2.3102763e-03 1.9347489e-02 9.7834229e-01]\n",
      " [1.3234056e-02 1.3512382e-01 8.5164213e-01]\n",
      " [7.5091397e-05 3.1412145e-01 6.8580347e-01]\n",
      " [1.2977095e-04 7.2069132e-01 2.7917892e-01]\n",
      " [4.6905223e-01 5.0482887e-01 2.6118927e-02]\n",
      " [2.4084112e-01 7.5717807e-01 1.9808023e-03]\n",
      " [6.5177768e-01 3.4677273e-01 1.4496183e-03]\n",
      " [7.6216078e-01 2.3767650e-01 1.6268159e-04]], cost: 0.3185668885707855\n",
      "step: 5800, hypothesis: [[2.0463380e-03 1.8323297e-02 9.7963035e-01]\n",
      " [1.2621329e-02 1.3421191e-01 8.5316676e-01]\n",
      " [6.4291642e-05 3.0959997e-01 6.9033575e-01]\n",
      " [1.1552420e-04 7.2438413e-01 2.7550033e-01]\n",
      " [4.6445203e-01 5.0989985e-01 2.5648078e-02]\n",
      " [2.3877633e-01 7.5937080e-01 1.8528686e-03]\n",
      " [6.5353346e-01 3.4511656e-01 1.3499472e-03]\n",
      " [7.6595283e-01 2.3390068e-01 1.4641421e-04]], cost: 0.3141491413116455\n",
      "step: 6000, hypothesis: [[1.81607576e-03 1.73719786e-02 9.80811954e-01]\n",
      " [1.20492252e-02 1.33280784e-01 8.54669988e-01]\n",
      " [5.51615412e-05 3.05252641e-01 6.94692194e-01]\n",
      " [1.03041915e-04 7.27953613e-01 2.71943361e-01]\n",
      " [4.59906638e-01 5.14894724e-01 2.51987018e-02]\n",
      " [2.36715376e-01 7.61548400e-01 1.73622742e-03]\n",
      " [6.55285239e-01 3.43455434e-01 1.25930237e-03]\n",
      " [7.69676805e-01 2.30191097e-01 1.32081579e-04]], cost: 0.3098602890968323\n",
      "step: 6200, hypothesis: [[1.6147336e-03 1.6486743e-02 9.8189849e-01]\n",
      " [1.1514331e-02 1.3233326e-01 8.5615242e-01]\n",
      " [4.7425816e-05 3.0106682e-01 6.9888574e-01]\n",
      " [9.2082038e-05 7.3140687e-01 2.6850101e-01]\n",
      " [4.5541841e-01 5.1981258e-01 2.4769064e-02]\n",
      " [2.3466210e-01 7.6370835e-01 1.6295691e-03]\n",
      " [6.5703279e-01 3.4179056e-01 1.1766452e-03]\n",
      " [7.7333468e-01 2.2654593e-01 1.1941291e-04]], cost: 0.3056935667991638\n",
      "step: 6400, hypothesis: [[1.4382883e-03 1.5661586e-02 9.8290008e-01]\n",
      " [1.1013473e-02 1.3137245e-01 8.5761410e-01]\n",
      " [4.0856659e-05 2.9703167e-01 7.0292747e-01]\n",
      " [8.2437939e-05 7.3475194e-01 2.6516563e-01]\n",
      " [4.5098770e-01 5.2465481e-01 2.4357527e-02]\n",
      " [2.3261784e-01 7.6585037e-01 1.5317837e-03]\n",
      " [6.5877521e-01 3.4012374e-01 1.1010801e-03]\n",
      " [7.7692598e-01 2.2296590e-01 1.0818164e-04]], cost: 0.30164238810539246\n",
      "step: 6600, hypothesis: [[1.2833356e-03 1.4891193e-02 9.8382550e-01]\n",
      " [1.0543906e-02 1.3040072e-01 8.5905540e-01]\n",
      " [3.5266468e-05 2.9313737e-01 7.0682734e-01]\n",
      " [7.3934483e-05 7.3799515e-01 2.6193097e-01]\n",
      " [4.4661489e-01 5.2942234e-01 2.3962786e-02]\n",
      " [2.3058544e-01 7.6797265e-01 1.4418978e-03]\n",
      " [6.6051245e-01 3.3845562e-01 1.0318392e-03]\n",
      " [7.8045195e-01 2.1944989e-01 9.8196899e-05]], cost: 0.2977008521556854\n",
      "step: 6800, hypothesis: [[1.1469853e-03 1.4170884e-02 9.8468208e-01]\n",
      " [1.0103123e-02 1.2942085e-01 8.6047602e-01]\n",
      " [3.0499268e-05 2.8937456e-01 7.1059501e-01]\n",
      " [6.6421897e-05 7.4114329e-01 2.5879025e-01]\n",
      " [4.4230032e-01 5.3411621e-01 2.3583455e-02]\n",
      " [2.2856680e-01 7.7007407e-01 1.3590864e-03]\n",
      " [6.6224343e-01 3.3678824e-01 9.6824893e-04]\n",
      " [7.8391314e-01 2.1599758e-01 8.9296642e-05]], cost: 0.2938636839389801\n",
      "step: 7000, hypothesis: [[1.0267688e-03 1.3496422e-02 9.8547679e-01]\n",
      " [9.6888663e-03 1.2843475e-01 8.6187637e-01]\n",
      " [2.6425712e-05 2.8573379e-01 7.1423984e-01]\n",
      " [5.9772137e-05 7.4420065e-01 2.5573957e-01]\n",
      " [4.3804368e-01 5.3873771e-01 2.3218641e-02]\n",
      " [2.2656399e-01 7.7215338e-01 1.2826325e-03]\n",
      " [6.6396868e-01 3.3512157e-01 9.0973673e-04]\n",
      " [7.8730994e-01 2.1260865e-01 8.1344668e-05]], cost: 0.2901257872581482\n",
      "step: 7200, hypothesis: [[9.2058367e-04 1.2864016e-02 9.8621535e-01]\n",
      " [9.2990361e-03 1.2744378e-01 8.6325711e-01]\n",
      " [2.2937646e-05 2.8220922e-01 7.1776783e-01]\n",
      " [5.3874359e-05 7.4717295e-01 2.5277320e-01]\n",
      " [4.3384778e-01 5.4328507e-01 2.2867156e-02]\n",
      " [2.2457416e-01 7.7421391e-01 1.2118899e-03]\n",
      " [6.6568643e-01 3.3345783e-01 8.5578422e-04]\n",
      " [7.9064298e-01 2.0928286e-01 7.4222618e-05]], cost: 0.2864830493927002\n",
      "step: 7400, hypothesis: [[8.2662085e-04 1.2270334e-02 9.8690307e-01]\n",
      " [8.9319050e-03 1.2645020e-01 8.6461794e-01]\n",
      " [1.9945564e-05 2.7879271e-01 7.2118735e-01]\n",
      " [4.8635262e-05 7.5006485e-01 2.4988656e-01]\n",
      " [4.2970973e-01 5.4776216e-01 2.2528090e-02]\n",
      " [2.2260378e-01 7.7624989e-01 1.1463091e-03]\n",
      " [6.6739732e-01 3.3179674e-01 8.0594234e-04]\n",
      " [7.9391390e-01 2.0601822e-01 6.7829438e-05]], cost: 0.28293097019195557\n",
      "step: 7600, hypothesis: [[7.4333261e-04 1.1712372e-02 9.8754430e-01]\n",
      " [8.5857157e-03 1.2545586e-01 8.6595845e-01]\n",
      " [1.7373917e-05 2.7547807e-01 7.2450459e-01]\n",
      " [4.3972701e-05 7.5288033e-01 2.4707571e-01]\n",
      " [4.2563069e-01 5.5216867e-01 2.2200665e-02]\n",
      " [2.2065201e-01 7.7826256e-01 1.0854136e-03]\n",
      " [6.6910106e-01 3.3013913e-01 7.5982377e-04]\n",
      " [7.9712290e-01 2.0281504e-01 6.2079511e-05]], cost: 0.27946603298187256\n",
      "step: 7800, hypothesis: [[6.6938082e-04 1.1187385e-02 9.8814327e-01]\n",
      " [8.2589332e-03 1.2446245e-01 8.6727858e-01]\n",
      " [1.5159530e-05 2.7225852e-01 7.2772628e-01]\n",
      " [3.9815950e-05 7.5562382e-01 2.4433638e-01]\n",
      " [4.2160818e-01 5.5650747e-01 2.1884294e-02]\n",
      " [2.1872035e-01 7.8025085e-01 1.0287811e-03]\n",
      " [6.7079544e-01 3.2848749e-01 7.1708718e-04]\n",
      " [8.0027020e-01 1.9967297e-01 5.6898134e-05]], cost: 0.2760844826698303\n",
      "step: 8000, hypothesis: [[6.0361275e-04 1.0692799e-02 9.8870355e-01]\n",
      " [7.9501783e-03 1.2347004e-01 8.6857980e-01]\n",
      " [1.3249233e-05 2.6912972e-01 7.3085707e-01]\n",
      " [3.6104040e-05 7.5829798e-01 2.4166588e-01]\n",
      " [4.1764542e-01 5.6077629e-01 2.1578271e-02]\n",
      " [2.1680710e-01 7.8221691e-01 9.7601634e-04]\n",
      " [6.7248237e-01 3.2684028e-01 6.7741831e-04]\n",
      " [8.0335826e-01 1.9658960e-01 5.2220104e-05]], cost: 0.2727828621864319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8200, hypothesis: [[5.4503302e-04 1.0226462e-02 9.8922849e-01]\n",
      " [7.6581882e-03 1.2248065e-01 8.6986119e-01]\n",
      " [1.1598477e-05 2.6608723e-01 7.3390120e-01]\n",
      " [3.2784075e-05 7.6090831e-01 2.3905888e-01]\n",
      " [4.1374025e-01 5.6497794e-01 2.1281879e-02]\n",
      " [2.1491362e-01 7.8415960e-01 9.2678313e-04]\n",
      " [6.7415935e-01 3.2520011e-01 6.4054196e-04]\n",
      " [8.0638587e-01 1.9356620e-01 4.7989022e-05]], cost: 0.26955848932266235\n",
      "step: 8400, hypothesis: [[4.9277744e-04 9.7863292e-03 9.8972088e-01]\n",
      " [7.3818257e-03 1.2149495e-01 8.7112319e-01]\n",
      " [1.0169514e-05 2.6312494e-01 7.3686492e-01]\n",
      " [2.9810319e-05 7.6345503e-01 2.3651512e-01]\n",
      " [4.0989178e-01 5.6911361e-01 2.0994553e-02]\n",
      " [2.1304142e-01 7.8607780e-01 8.8078307e-04]\n",
      " [6.7582774e-01 3.2356602e-01 6.0621463e-04]\n",
      " [8.0935544e-01 1.9060038e-01 4.4155557e-05]], cost: 0.2664080858230591\n",
      "step: 8600, hypothesis: [[4.4609228e-04 9.3705188e-03 9.9018341e-01]\n",
      " [7.1200095e-03 1.2051394e-01 8.7236601e-01]\n",
      " [8.9305186e-06 2.6024005e-01 7.3975104e-01]\n",
      " [2.7142545e-05 7.6594311e-01 2.3402978e-01]\n",
      " [4.0609935e-01 5.7318473e-01 2.0715876e-02]\n",
      " [2.1119070e-01 7.8797156e-01 8.3774445e-04]\n",
      " [6.7748594e-01 3.2193980e-01 5.7421776e-04]\n",
      " [8.1226724e-01 1.8769205e-01 4.0677020e-05]], cost: 0.2633291482925415\n",
      "step: 8800, hypothesis: [[4.04325459e-04 8.97734053e-03 9.90618408e-01]\n",
      " [6.87177200e-03 1.19538285e-01 8.73589933e-01]\n",
      " [7.85437442e-06 2.57427990e-01 7.42564142e-01]\n",
      " [2.47456283e-05 7.68374562e-01 2.31600672e-01]\n",
      " [4.02364343e-01 5.77190340e-01 2.04453804e-02]\n",
      " [2.09361658e-01 7.89840877e-01 7.97422661e-04]\n",
      " [6.79135561e-01 3.20320040e-01 5.44356299e-04]\n",
      " [8.15123022e-01 1.84839442e-01 3.75155578e-05]], cost: 0.2603189945220947\n",
      "step: 9000, hypothesis: [[3.6690783e-04 8.6052204e-03 9.9102789e-01]\n",
      " [6.6362121e-03 1.1856837e-01 8.7479550e-01]\n",
      " [6.9182315e-06 2.5468564e-01 7.4530745e-01]\n",
      " [2.2589105e-05 7.7075148e-01 2.2922592e-01]\n",
      " [3.9868510e-01 5.8113217e-01 2.0182693e-02]\n",
      " [2.0755337e-01 7.9168695e-01 7.5960602e-04]\n",
      " [6.8077481e-01 3.1870869e-01 5.1645632e-04]\n",
      " [8.1792313e-01 1.8204232e-01 3.4638259e-05]], cost: 0.2573752999305725\n",
      "step: 9200, hypothesis: [[3.33340693e-04 8.25280044e-03 9.91413832e-01]\n",
      " [6.41250238e-03 1.17605284e-01 8.75982225e-01]\n",
      " [6.10256302e-06 2.52009273e-01 7.47984588e-01]\n",
      " [2.06460827e-05 7.73076892e-01 2.26902470e-01]\n",
      " [3.95060390e-01 5.85012376e-01 1.99272502e-02]\n",
      " [2.05767825e-01 7.93508112e-01 7.24093814e-04]\n",
      " [6.82404637e-01 3.17105025e-01 4.90354141e-04]\n",
      " [8.20668936e-01 1.79299042e-01 3.20154941e-05]], cost: 0.2544955015182495\n",
      "step: 9400, hypothesis: [[3.0318840e-04 7.9187434e-03 9.9177814e-01]\n",
      " [6.1998786e-03 1.1664915e-01 8.7715101e-01]\n",
      " [5.3907538e-06 2.4939553e-01 7.5059909e-01]\n",
      " [1.8893013e-05 7.7535200e-01 2.2462915e-01]\n",
      " [3.9148894e-01 5.8883226e-01 1.9678796e-02]\n",
      " [2.0400405e-01 7.9530525e-01 6.9071190e-04]\n",
      " [6.8402261e-01 3.1551141e-01 4.6591219e-04]\n",
      " [8.2336032e-01 1.7661005e-01 2.9621948e-05]], cost: 0.25167781114578247\n",
      "step: 9600, hypothesis: [[2.7606895e-04 7.6018516e-03 9.9212205e-01]\n",
      " [5.9976620e-03 1.1570064e-01 8.7830162e-01]\n",
      " [4.7686640e-06 2.4684229e-01 7.5315291e-01]\n",
      " [1.7309258e-05 7.7757967e-01 2.2240297e-01]\n",
      " [3.8797301e-01 5.9258991e-01 1.9437047e-02]\n",
      " [2.0226201e-01 7.9707867e-01 6.5929780e-04]\n",
      " [6.8563205e-01 3.1392497e-01 4.4299886e-04]\n",
      " [8.2600021e-01 1.7397225e-01 2.7434484e-05]], cost: 0.24891994893550873\n",
      "step: 9800, hypothesis: [[2.5164708e-04 7.3010470e-03 9.9244726e-01]\n",
      " [5.8051753e-03 1.1475989e-01 8.7943494e-01]\n",
      " [4.2241431e-06 2.4434613e-01 7.5564963e-01]\n",
      " [1.5876558e-05 7.7976084e-01 2.2022320e-01]\n",
      " [3.8450834e-01 5.9629005e-01 1.9201595e-02]\n",
      " [2.0054212e-01 7.9882818e-01 6.2970782e-04]\n",
      " [6.8722957e-01 3.1234890e-01 4.2149631e-04]\n",
      " [8.2858771e-01 1.7138693e-01 2.5432941e-05]], cost: 0.2462199330329895\n",
      "step: 10000, hypothesis: [[2.2962803e-04 7.0153023e-03 9.9275512e-01]\n",
      " [5.6218556e-03 1.1382768e-01 8.8055044e-01]\n",
      " [3.7468510e-06 2.4190536e-01 7.5809091e-01]\n",
      " [1.4578898e-05 7.8189838e-01 2.1808700e-01]\n",
      " [3.8109747e-01 5.9993029e-01 1.8972242e-02]\n",
      " [1.9884445e-01 8.0055374e-01 6.0180912e-04]\n",
      " [6.8881750e-01 3.1078124e-01 4.0129977e-04]\n",
      " [8.3112568e-01 1.6885069e-01 2.3599297e-05]], cost: 0.2435760498046875\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "'''\n",
    "변수 4개를 통해 나오는 1개 결과는 (0, 1, 2)세가지 중 하나. \n",
    "y_data 는 one-hot encoding 방식으로 표현된 결과 \n",
    "'''\n",
    "#data \n",
    "x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "#y_data = [[2], [2], [2], [1], [1], [1], [0], [0]]\n",
    "nb_classes = 3\n",
    "#node\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1, nb_classes]), name='bias')\n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "#cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, lables=y_data)\n",
    "#cost = tf.reduce_mean(cost_i)\n",
    "\"\"\"\n",
    "과정\n",
    "hypothesis를 실행했을 때 Nx3 행렬. y_data의 형태. axis=1 : 2차원 제거하면서 1차원에 합한다. \n",
    "Y * -log(hypothesis)는 가설이 올바를 때 0에 수렴. \n",
    "y_data가 one-hot형태이기 때문에 binary classification의 0과 1을 모두 확인할 수 있음. \n",
    "multi classification은 binary classification의 조합. \n",
    "\"\"\"\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cost_val, h_val, _ = sess.run([cost, hypothesis, train], feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "        if step%200==0:\n",
    "            print('step: {0}, hypothesis: {1}, cost: {2}'.format(step, h_val, cost_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n",
      "Step:     0\tLoss: 4.906\tAcc: 9.90%\n",
      "Step:   100\tLoss: 0.605\tAcc: 86.14%\n",
      "Step:   200\tLoss: 0.380\tAcc: 88.12%\n",
      "Step:   300\tLoss: 0.281\tAcc: 91.09%\n",
      "Step:   400\tLoss: 0.221\tAcc: 93.07%\n",
      "Step:   500\tLoss: 0.181\tAcc: 99.01%\n",
      "Step:   600\tLoss: 0.153\tAcc: 100.00%\n",
      "Step:   700\tLoss: 0.132\tAcc: 100.00%\n",
      "Step:   800\tLoss: 0.117\tAcc: 100.00%\n",
      "Step:   900\tLoss: 0.105\tAcc: 100.00%\n",
      "Step:  1000\tLoss: 0.095\tAcc: 100.00%\n",
      "Step:  1100\tLoss: 0.087\tAcc: 100.00%\n",
      "Step:  1200\tLoss: 0.080\tAcc: 100.00%\n",
      "Step:  1300\tLoss: 0.074\tAcc: 100.00%\n",
      "Step:  1400\tLoss: 0.069\tAcc: 100.00%\n",
      "Step:  1500\tLoss: 0.065\tAcc: 100.00%\n",
      "Step:  1600\tLoss: 0.061\tAcc: 100.00%\n",
      "Step:  1700\tLoss: 0.058\tAcc: 100.00%\n",
      "Step:  1800\tLoss: 0.055\tAcc: 100.00%\n",
      "Step:  1900\tLoss: 0.052\tAcc: 100.00%\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 5, True Y: 5\n",
      "[True] Prediction: 4, True Y: 4\n",
      "[True] Prediction: 4, True Y: 4\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 5, True Y: 5\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 5, True Y: 5\n",
      "[True] Prediction: 5, True Y: 5\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 5, True Y: 5\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 5, True Y: 5\n",
      "[True] Prediction: 4, True Y: 4\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 2, True Y: 2\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 2, True Y: 2\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 2, True Y: 2\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 5, True Y: 5\n",
      "[True] Prediction: 4, True Y: 4\n",
      "[True] Prediction: 2, True Y: 2\n",
      "[True] Prediction: 2, True Y: 2\n",
      "[True] Prediction: 3, True Y: 3\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 1, True Y: 1\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 5, True Y: 5\n",
      "[True] Prediction: 0, True Y: 0\n",
      "[True] Prediction: 6, True Y: 6\n",
      "[True] Prediction: 1, True Y: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#for reproducibility\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "nb_classes = 7\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot:\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape one_hot:\", Y_one_hot)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                                 labels=tf.stop_gradient([Y_one_hot])))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "        if step%100==0:\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "            print(\"Step: {:5}\\tLoss: {:.3f}\\tAcc: {:.2%}\".format(step, loss, acc))\n",
    "    \n",
    "    pred = sess.run(prediction, feed_dict={X:x_data})\n",
    "    for p, y in zip(pred, y_data.flatten()):\n",
    "        print(\"[{}] Prediction: {}, True Y: {}\".format(p == int(y), p, int(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
