{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n여러 변수를 통해 하나의 결과가 나오는 케이스\\n\\n가설함수 H(x1, x2, x3, ... xn) : w1x1+ w2x2 + w3x3+ ... wnxn + b\\ncost(x1, x2, ... xn) : tf.reduce_mean(tf.square(H(x1 ... xn) - y))\\n\\n변수 갯수가 너무 많다. 효율적으로 관리할 필요생김. -> matrix\\n\\n(3*1 matrx) (1*3 matrix) = 1*1 matrix !\\n3*1 을 x1, x2, x3 라 가정하고 1*3을 w1, w2, w3라하면 matrix X * matrix W = w1x1 + w2x2 + w3x3 가 되므로\\n\\n가설함수 H(X) = X * W이다\\n\\n예시)\\n1학기 중간, 1학기 기말, 2학기 중간, 성적이 주어질 때 2학기 기말 점수 예측\\nx1          x2          x3          y\\n\\n            ...\\nn개의 인스턴스가 있다고 할 때 3*n 행렬곱 1*3 은 n*1이 되며 결과적으로 각 인스턴스들의 결과값의 행렬이 된다.\\n\\nn개 변수 m개 인스턴스라 할 때 p개의 결과가 나온다고 하면\\n\\n[n, m] * W[?, ?] = [n, p]  -> W = [m, p]\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "여러 변수를 통해 하나의 결과가 나오는 케이스\n",
    "\n",
    "가설함수 H(x1, x2, x3, ... xn) : w1x1+ w2x2 + w3x3+ ... wnxn + b\n",
    "cost(x1, x2, ... xn) : tf.reduce_mean(tf.square(H(x1 ... xn) - y))\n",
    "\n",
    "변수 갯수가 너무 많다. 효율적으로 관리할 필요생김. -> matrix\n",
    "\n",
    "(3*1 matrx) (1*3 matrix) = 1*1 matrix !\n",
    "3*1 을 x1, x2, x3 라 가정하고 1*3을 w1, w2, w3라하면 matrix X * matrix W = w1x1 + w2x2 + w3x3 가 되므로\n",
    "\n",
    "가설함수 H(X) = X * W이다\n",
    "\n",
    "예시)\n",
    "1학기 중간, 1학기 기말, 2학기 중간, 성적이 주어질 때 2학기 기말 점수 예측\n",
    "x1          x2          x3          y\n",
    "\n",
    "            ...\n",
    "n개의 인스턴스가 있다고 할 때 3*n 행렬곱 1*3 은 n*1이 되며 결과적으로 각 인스턴스들의 결과값의 행렬이 된다.\n",
    "\n",
    "n개 변수 m개 인스턴스라 할 때 p개의 결과가 나온다고 하면\n",
    "\n",
    "[n, m] * W[?, ?] = [n, p]  -> W = [m, p]\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#예상 코드\\nimport tensorflow as tf\\n\\nX = [[73, 80, 75],\\n     [93, 88, 93],\\n     [89, 90, 91],\\n     [96, 98, 100]]\\nY = [152, 185, 180, 196]\\n\\nW = tf.Variable([tf.random_normal[1], tf.random_normal[1], tf.random_normal[1], tf.random_normal[1]])\\nhypothesis = X * W\\n\\ncost = tf.reduce_mean(tf.square(hypothesis - Y))\\n\\noptimize = tf.train.GradientDescentOptimizer(learning_rate=0.1)\\ntrain = optimize.minimize(cost)\\n\\nsess = tf.Session()\\nsess.run(tf.global_variables_initializer())\\n\\nfor step in range(2001):\\n    sess.run(train)\\n    if step%20 ==0 :\\n        print(step, sess.run(W), sess.run(cost))\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#예상 코드\n",
    "import tensorflow as tf\n",
    "\n",
    "X = [[73, 80, 75],\n",
    "     [93, 88, 93],\n",
    "     [89, 90, 91],\n",
    "     [96, 98, 100]]\n",
    "Y = [152, 185, 180, 196]\n",
    "\n",
    "W = tf.Variable([tf.random_normal[1], tf.random_normal[1], tf.random_normal[1], tf.random_normal[1]])\n",
    "hypothesis = X * W\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimize.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step%20 ==0 :\n",
    "        print(step, sess.run(W), sess.run(cost))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, cost: 11875.712890625, prediction(hypothesis): [52.22256 69.46746 66.26176 73.02702 52.46498]\n",
      "step: 20, cost: 2.8741986751556396, prediction(hypothesis): [148.90266 185.67056 180.76399 197.71187 141.09796]\n",
      "step: 40, cost: 2.8676512241363525, prediction(hypothesis): [148.90952 185.6682  180.76573 197.7134  141.09523]\n",
      "step: 60, cost: 2.8611717224121094, prediction(hypothesis): [148.91545 185.66481 180.76643 197.71379 141.09174]\n",
      "step: 80, cost: 2.854736566543579, prediction(hypothesis): [148.92137 185.66145 180.76714 197.7142  141.08829]\n",
      "step: 100, cost: 2.8483471870422363, prediction(hypothesis): [148.92726 185.6581  180.76784 197.71458 141.08482]\n",
      "step: 120, cost: 2.841991424560547, prediction(hypothesis): [148.93312 185.65475 180.76851 197.71495 141.08139]\n",
      "step: 140, cost: 2.835693836212158, prediction(hypothesis): [148.93895 185.65143 180.7692  197.71532 141.07799]\n",
      "step: 160, cost: 2.8294341564178467, prediction(hypothesis): [148.94476 185.64813 180.76988 197.71568 141.07462]\n",
      "step: 180, cost: 2.8232295513153076, prediction(hypothesis): [148.95055 185.64484 180.77055 197.71605 141.07124]\n",
      "step: 200, cost: 2.8170809745788574, prediction(hypothesis): [148.95628 185.64156 180.77121 197.7164  141.0679 ]\n",
      "step: 220, cost: 2.810960292816162, prediction(hypothesis): [148.962   185.63829 180.77188 197.71672 141.06456]\n",
      "step: 240, cost: 2.8048784732818604, prediction(hypothesis): [148.96773 185.63506 180.77255 197.71709 141.06128]\n",
      "step: 260, cost: 2.7988460063934326, prediction(hypothesis): [148.9734  185.63184 180.77321 197.71742 141.05801]\n",
      "step: 280, cost: 2.7928807735443115, prediction(hypothesis): [148.97903 185.62862 180.77385 197.71774 141.05473]\n",
      "step: 300, cost: 2.7869081497192383, prediction(hypothesis): [148.98468 185.62543 180.7745  197.71806 141.05151]\n",
      "step: 320, cost: 2.7810046672821045, prediction(hypothesis): [148.99028 185.62225 180.77516 197.71837 141.0483 ]\n",
      "step: 340, cost: 2.775153398513794, prediction(hypothesis): [148.99583 185.61906 180.77577 197.71867 141.04509]\n",
      "step: 360, cost: 2.7693402767181396, prediction(hypothesis): [149.00139 185.61594 180.77643 197.71898 141.04192]\n",
      "step: 380, cost: 2.763545036315918, prediction(hypothesis): [149.00693 185.61281 180.77707 197.71927 141.03876]\n",
      "step: 400, cost: 2.7578141689300537, prediction(hypothesis): [149.01242 185.60968 180.77768 197.71957 141.03563]\n",
      "step: 420, cost: 2.7521026134490967, prediction(hypothesis): [149.01791 185.60658 180.77834 197.71986 141.03253]\n",
      "step: 440, cost: 2.7464475631713867, prediction(hypothesis): [149.02335 185.6035  180.77893 197.72012 141.02943]\n",
      "step: 460, cost: 2.740825653076172, prediction(hypothesis): [149.02878 185.60042 180.77956 197.7204  141.02635]\n",
      "step: 480, cost: 2.7352349758148193, prediction(hypothesis): [149.0342  185.59738 180.78018 197.72067 141.02332]\n",
      "step: 500, cost: 2.7297027111053467, prediction(hypothesis): [149.03955 185.59433 180.78076 197.72092 141.02026]\n",
      "step: 520, cost: 2.7242038249969482, prediction(hypothesis): [149.0449  185.59131 180.78139 197.72118 141.01726]\n",
      "step: 540, cost: 2.7187294960021973, prediction(hypothesis): [149.05025 185.5883  180.78198 197.72144 141.01427]\n",
      "step: 560, cost: 2.713303565979004, prediction(hypothesis): [149.05554 185.58528 180.78258 197.72166 141.01128]\n",
      "step: 580, cost: 2.7078938484191895, prediction(hypothesis): [149.06085 185.58232 180.78319 197.72192 141.00835]\n",
      "step: 600, cost: 2.7025465965270996, prediction(hypothesis): [149.0661  185.57935 180.78377 197.72215 141.0054 ]\n",
      "step: 620, cost: 2.6972243785858154, prediction(hypothesis): [149.07132 185.57639 180.78433 197.72235 141.00247]\n",
      "step: 640, cost: 2.6919448375701904, prediction(hypothesis): [149.07655 185.57347 180.78494 197.72261 140.9996 ]\n",
      "step: 660, cost: 2.686687707901001, prediction(hypothesis): [149.08174 185.57054 180.78552 197.72281 140.99672]\n",
      "step: 680, cost: 2.681485652923584, prediction(hypothesis): [149.08688 185.56761 180.78607 197.723   140.99385]\n",
      "step: 700, cost: 2.676301956176758, prediction(hypothesis): [149.09204 185.56473 180.78665 197.72322 140.99101]\n",
      "step: 720, cost: 2.6711537837982178, prediction(hypothesis): [149.09717 185.56184 180.78723 197.72342 140.98819]\n",
      "step: 740, cost: 2.6660447120666504, prediction(hypothesis): [149.10226 185.55898 180.7878  197.72362 140.9854 ]\n",
      "step: 760, cost: 2.6609582901000977, prediction(hypothesis): [149.10735 185.55612 180.78835 197.7238  140.9826 ]\n",
      "step: 780, cost: 2.655945301055908, prediction(hypothesis): [149.11238 185.5533  180.78893 197.72398 140.97984]\n",
      "step: 800, cost: 2.65090012550354, prediction(hypothesis): [149.11743 185.55048 180.78946 197.72415 140.9771 ]\n",
      "step: 820, cost: 2.6459262371063232, prediction(hypothesis): [149.12242 185.54764 180.79001 197.7243  140.97437]\n",
      "step: 840, cost: 2.6409733295440674, prediction(hypothesis): [149.12743 185.54486 180.79056 197.72449 140.97166]\n",
      "step: 860, cost: 2.63606858253479, prediction(hypothesis): [149.13239 185.54207 180.7911  197.72466 140.96898]\n",
      "step: 880, cost: 2.63118839263916, prediction(hypothesis): [149.13733 185.5393  180.79164 197.72481 140.9663 ]\n",
      "step: 900, cost: 2.626330852508545, prediction(hypothesis): [149.14226 185.53656 180.79219 197.72496 140.96365]\n",
      "step: 920, cost: 2.6215243339538574, prediction(hypothesis): [149.14714 185.5338  180.79271 197.7251  140.961  ]\n",
      "step: 940, cost: 2.61674427986145, prediction(hypothesis): [149.15201 185.53107 180.79323 197.72523 140.95837]\n",
      "step: 960, cost: 2.6119697093963623, prediction(hypothesis): [149.15689 185.52837 180.79378 197.72539 140.9558 ]\n",
      "step: 980, cost: 2.6072583198547363, prediction(hypothesis): [149.16171 185.52565 180.7943  197.72551 140.95319]\n",
      "step: 1000, cost: 2.6025547981262207, prediction(hypothesis): [149.16652 185.52295 180.7948  197.72562 140.9506 ]\n",
      "step: 1020, cost: 2.5978682041168213, prediction(hypothesis): [149.17133 185.52028 180.79532 197.72574 140.94807]\n",
      "step: 1040, cost: 2.5932579040527344, prediction(hypothesis): [149.17609 185.51762 180.79584 197.72586 140.94553]\n",
      "step: 1060, cost: 2.5886240005493164, prediction(hypothesis): [149.18085 185.51498 180.79634 197.72597 140.94304]\n",
      "step: 1080, cost: 2.5840845108032227, prediction(hypothesis): [149.18556 185.51234 180.79686 197.72609 140.94052]\n",
      "step: 1100, cost: 2.5795187950134277, prediction(hypothesis): [149.19028 185.50972 180.79735 197.72618 140.93803]\n",
      "step: 1120, cost: 2.574965715408325, prediction(hypothesis): [149.19498 185.5071  180.79785 197.72626 140.93558]\n",
      "step: 1140, cost: 2.570510149002075, prediction(hypothesis): [149.19962 185.50449 180.79834 197.72635 140.9331 ]\n",
      "step: 1160, cost: 2.5660147666931152, prediction(hypothesis): [149.20428 185.5019  180.79883 197.72644 140.9307 ]\n",
      "step: 1180, cost: 2.5615811347961426, prediction(hypothesis): [149.20891 185.49934 180.79933 197.72652 140.92828]\n",
      "step: 1200, cost: 2.557154893875122, prediction(hypothesis): [149.21352 185.49677 180.7998  197.72658 140.92587]\n",
      "step: 1220, cost: 2.5527901649475098, prediction(hypothesis): [149.21811 185.49423 180.80031 197.72668 140.92351]\n",
      "step: 1240, cost: 2.548419237136841, prediction(hypothesis): [149.22267 185.49168 180.80077 197.72673 140.92113]\n",
      "step: 1260, cost: 2.5440902709960938, prediction(hypothesis): [149.2272  185.48915 180.80124 197.72678 140.91878]\n",
      "step: 1280, cost: 2.539762020111084, prediction(hypothesis): [149.23177 185.48666 180.80173 197.72685 140.91646]\n",
      "step: 1300, cost: 2.535510301589966, prediction(hypothesis): [149.23625 185.48415 180.8022  197.72691 140.91414]\n",
      "step: 1320, cost: 2.531237840652466, prediction(hypothesis): [149.24074 185.48164 180.80266 197.72693 140.91182]\n",
      "step: 1340, cost: 2.5270028114318848, prediction(hypothesis): [149.24522 185.47919 180.80313 197.72699 140.90955]\n",
      "step: 1360, cost: 2.522812604904175, prediction(hypothesis): [149.24966 185.47672 180.80359 197.72704 140.90727]\n",
      "step: 1380, cost: 2.5186171531677246, prediction(hypothesis): [149.2541  185.47427 180.80405 197.72707 140.90503]\n",
      "step: 1400, cost: 2.5144619941711426, prediction(hypothesis): [149.2585  185.47182 180.80449 197.72707 140.90277]\n",
      "step: 1420, cost: 2.5103440284729004, prediction(hypothesis): [149.2629  185.4694  180.80495 197.72711 140.90056]\n",
      "step: 1440, cost: 2.506237506866455, prediction(hypothesis): [149.26727 185.467   180.8054  197.72714 140.89836]\n",
      "step: 1460, cost: 2.502155303955078, prediction(hypothesis): [149.2716  185.46457 180.80583 197.72713 140.89615]\n",
      "step: 1480, cost: 2.4980931282043457, prediction(hypothesis): [149.27596 185.46219 180.80627 197.72716 140.89398]\n",
      "step: 1500, cost: 2.494042158126831, prediction(hypothesis): [149.28029 185.45981 180.80672 197.72717 140.89183]\n",
      "step: 1520, cost: 2.4900383949279785, prediction(hypothesis): [149.28458 185.45743 180.80716 197.72716 140.88966]\n",
      "step: 1540, cost: 2.4860525131225586, prediction(hypothesis): [149.28886 185.45508 180.8076  197.72717 140.88754]\n",
      "step: 1560, cost: 2.4820799827575684, prediction(hypothesis): [149.29312 185.45273 180.80801 197.72716 140.88542]\n",
      "step: 1580, cost: 2.4781265258789062, prediction(hypothesis): [149.29736 185.45038 180.80844 197.72714 140.88333]\n",
      "step: 1600, cost: 2.4742140769958496, prediction(hypothesis): [149.30159 185.44807 180.80888 197.72713 140.88124]\n",
      "step: 1620, cost: 2.470299243927002, prediction(hypothesis): [149.3058  185.44576 180.8093  197.7271  140.87917]\n",
      "step: 1640, cost: 2.4664342403411865, prediction(hypothesis): [149.30998 185.44345 180.80971 197.72708 140.8771 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1660, cost: 2.4625821113586426, prediction(hypothesis): [149.31415 185.44116 180.81012 197.72705 140.87506]\n",
      "step: 1680, cost: 2.4587502479553223, prediction(hypothesis): [149.3183  185.43887 180.81053 197.72702 140.87303]\n",
      "step: 1700, cost: 2.4549167156219482, prediction(hypothesis): [149.32246 185.43663 180.81097 197.72699 140.87103]\n",
      "step: 1720, cost: 2.4511373043060303, prediction(hypothesis): [149.32655 185.43434 180.81136 197.72694 140.86903]\n",
      "step: 1740, cost: 2.4473748207092285, prediction(hypothesis): [149.33064 185.43208 180.81175 197.72688 140.86702]\n",
      "step: 1760, cost: 2.443605899810791, prediction(hypothesis): [149.33475 185.42987 180.81216 197.72685 140.86508]\n",
      "step: 1780, cost: 2.439878463745117, prediction(hypothesis): [149.3388  185.42763 180.81256 197.72679 140.86311]\n",
      "step: 1800, cost: 2.4361631870269775, prediction(hypothesis): [149.34286 185.42542 180.81296 197.72675 140.86118]\n",
      "step: 1820, cost: 2.432485580444336, prediction(hypothesis): [149.34688 185.4232  180.81335 197.72667 140.85924]\n",
      "step: 1840, cost: 2.428805112838745, prediction(hypothesis): [149.3509  185.42102 180.81374 197.72661 140.85732]\n",
      "step: 1860, cost: 2.4251532554626465, prediction(hypothesis): [149.35492 185.41885 180.81415 197.72656 140.85544]\n",
      "step: 1880, cost: 2.421522378921509, prediction(hypothesis): [149.35889 185.41667 180.81453 197.72647 140.85355]\n",
      "step: 1900, cost: 2.4179234504699707, prediction(hypothesis): [149.36282 185.41449 180.81488 197.72638 140.85165]\n",
      "step: 1920, cost: 2.4143154621124268, prediction(hypothesis): [149.3668  185.41237 180.81529 197.72632 140.84981]\n",
      "step: 1940, cost: 2.410740375518799, prediction(hypothesis): [149.37073 185.41022 180.81566 197.72623 140.84796]\n",
      "step: 1960, cost: 2.407196521759033, prediction(hypothesis): [149.37462 185.40807 180.81602 197.72612 140.84612]\n",
      "step: 1980, cost: 2.40366268157959, prediction(hypothesis): [149.37852 185.40594 180.8164  197.72604 140.8443 ]\n",
      "step: 2000, cost: 2.4001340866088867, prediction(hypothesis): [149.38242 185.40384 180.81677 197.72595 140.8425 ]\n"
     ]
    }
   ],
   "source": [
    "#단순코딩\n",
    "import tensorflow as tf\n",
    "\n",
    "#잘못된 데이터\n",
    "'''\n",
    "x1_data = [73, 80, 75]\n",
    "x2_data = [93, 88, 93]\n",
    "x3_data = [89, 90, 91]\n",
    "x4_data = [96, 98, 100]\n",
    "y_data = [152, 185, 180, 196]\n",
    "\n",
    "변수3개에서 한 결과가 나오는 상황에서 변수를 4개 만들었다.\n",
    "\n",
    "세로로 읽어서 한 인스턴스가 된다. \n",
    "i번째 계산에서 x1[i], x2[i], x3[i], y[i]가 들어가야하므로.\n",
    "'''\n",
    "#올바른 데이터\n",
    "x1_data = [73., 93., 89., 96., 73.]\n",
    "x2_data = [80., 88., 90., 98., 66.]\n",
    "x3_data = [75., 93., 91., 100., 70.]\n",
    "y_data = [152., 185., 180., 196., 142.]\n",
    "\n",
    "#노드\n",
    "x1 = tf.placeholder(tf.float32)\n",
    "x2 = tf.placeholder(tf.float32)\n",
    "x3 = tf.placeholder(tf.float32)\n",
    "\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "#계수 W\n",
    "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
    "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
    "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
    "\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "\n",
    "#가설함수\n",
    "hypothesis = x1*w1 + x2*w2 + x3*w3 + b\n",
    "\n",
    "#목적\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "#최소기울기\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimize.minimize(cost)\n",
    "\n",
    "#실행\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run([cost, hypothesis, train],\n",
    "                                   feed_dict={x1: x1_data, x2: x2_data, x3: x3_data, y: y_data})\n",
    "    if step%20==0:\n",
    "        print('step: {0}, cost: {1}, prediction(hypothesis): {2}'.format(step, cost_val, hy_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost: 2973.621826171875, prediction(hypothesis): [[105.142685]\n",
      " [128.22723 ]\n",
      " [125.57147 ]\n",
      " [136.72717 ]]\n",
      "100 cost: 0.44948500394821167, prediction(hypothesis): [[151.6001 ]\n",
      " [184.06282]\n",
      " [180.59132]\n",
      " [196.64035]]\n",
      "200 cost: 0.44754770398139954, prediction(hypothesis): [[151.5954 ]\n",
      " [184.06813]\n",
      " [180.59137]\n",
      " [196.63905]]\n",
      "300 cost: 0.4456666111946106, prediction(hypothesis): [[151.5908 ]\n",
      " [184.07332]\n",
      " [180.5914 ]\n",
      " [196.63776]]\n",
      "400 cost: 0.4438454806804657, prediction(hypothesis): [[151.5863 ]\n",
      " [184.0784 ]\n",
      " [180.59143]\n",
      " [196.63647]]\n",
      "500 cost: 0.44205737113952637, prediction(hypothesis): [[151.58192]\n",
      " [184.0834 ]\n",
      " [180.59148]\n",
      " [196.63518]]\n",
      "600 cost: 0.44031035900115967, prediction(hypothesis): [[151.57764]\n",
      " [184.08832]\n",
      " [180.59149]\n",
      " [196.6339 ]]\n",
      "700 cost: 0.438615620136261, prediction(hypothesis): [[151.57346]\n",
      " [184.09315]\n",
      " [180.59155]\n",
      " [196.63263]]\n",
      "800 cost: 0.436960369348526, prediction(hypothesis): [[151.56935]\n",
      " [184.09789]\n",
      " [180.59157]\n",
      " [196.63136]]\n",
      "900 cost: 0.43534305691719055, prediction(hypothesis): [[151.56535]\n",
      " [184.10252]\n",
      " [180.5916 ]\n",
      " [196.63008]]\n",
      "1000 cost: 0.4337675869464874, prediction(hypothesis): [[151.56146]\n",
      " [184.10707]\n",
      " [180.59163]\n",
      " [196.62881]]\n",
      "1100 cost: 0.4322272837162018, prediction(hypothesis): [[151.55766]\n",
      " [184.11156]\n",
      " [180.59166]\n",
      " [196.62758]]\n",
      "1200 cost: 0.4307236671447754, prediction(hypothesis): [[151.55394]\n",
      " [184.11595]\n",
      " [180.59169]\n",
      " [196.62633]]\n",
      "1300 cost: 0.4292517304420471, prediction(hypothesis): [[151.55032]\n",
      " [184.12027]\n",
      " [180.59172]\n",
      " [196.62509]]\n",
      "1400 cost: 0.42781251668930054, prediction(hypothesis): [[151.54678]\n",
      " [184.12451]\n",
      " [180.59175]\n",
      " [196.62386]]\n",
      "1500 cost: 0.42640167474746704, prediction(hypothesis): [[151.54333]\n",
      " [184.12868]\n",
      " [180.59178]\n",
      " [196.62262]]\n",
      "1600 cost: 0.425021767616272, prediction(hypothesis): [[151.53996]\n",
      " [184.13277]\n",
      " [180.59181]\n",
      " [196.62138]]\n",
      "1700 cost: 0.4236745536327362, prediction(hypothesis): [[151.53667]\n",
      " [184.1368 ]\n",
      " [180.59186]\n",
      " [196.62016]]\n",
      "1800 cost: 0.4223625063896179, prediction(hypothesis): [[151.53346]\n",
      " [184.14075]\n",
      " [180.5919 ]\n",
      " [196.61897]]\n",
      "1900 cost: 0.42107343673706055, prediction(hypothesis): [[151.53033]\n",
      " [184.14459]\n",
      " [180.59192]\n",
      " [196.61775]]\n",
      "2000 cost: 0.4198051691055298, prediction(hypothesis): [[151.52727]\n",
      " [184.1484 ]\n",
      " [180.59195]\n",
      " [196.61655]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 잘못된 데이터\n",
    "'''\n",
    "x_data = [[73., 93., 89., 96.], [80., 88., 90., 98.], [75., 93., 91., 100.]]\n",
    "y_data = [152, 185, 180, 196]\n",
    "'''\n",
    "#올바른 데이터, x: x_data로 한번에 변수 3개가 모두 들어가야 하기 때문.\n",
    "x_data = [[73., 80., 75.], [93., 88., 93.], [89., 90., 91.], [96., 98., 100.]]\n",
    "#y: y_data로 결과는 1개이므로 한번에 1개씩 들어가야 하기 때문\n",
    "y_data = [[152.], [185.], [180.], [196.]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 1]), name='weight') # 1개 요소로된 요소가 3개이다. 즉, 3행 1열 w = [ [w1], [w2], [w3] ]\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "#행렬 곱 해야함\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimize.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    if step%100==0:\n",
    "        print(step, 'cost: {0}, prediction(hypothesis): {1}'.format(cost_val, hy_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
