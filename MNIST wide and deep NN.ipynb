{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-fe4008e7c1e9>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ysoh\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 0001, Cost: 1.890700853\n",
      "Epoch: 0002, Cost: 1.346194338\n",
      "Epoch: 0003, Cost: 1.110778347\n",
      "Epoch: 0004, Cost: 0.964712243\n",
      "Epoch: 0005, Cost: 0.883120232\n",
      "Epoch: 0006, Cost: 0.832375485\n",
      "Epoch: 0007, Cost: 0.765933432\n",
      "Epoch: 0008, Cost: 0.698319274\n",
      "Epoch: 0009, Cost: 0.657790026\n",
      "Epoch: 0010, Cost: 0.629163487\n",
      "Epoch: 0011, Cost: 0.602538119\n",
      "Epoch: 0012, Cost: 0.570517757\n",
      "Epoch: 0013, Cost: 0.538830872\n",
      "Epoch: 0014, Cost: 0.515377301\n",
      "Epoch: 0015, Cost: 0.497230889\n",
      "Learning finished\n",
      "Accuracy:  0.8448\n",
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n은닉층을 2개 늘리고\\n\\n출력은 4배에서 2배 1배로 출력되도록 설계함\\n\\n은닉층이 없을 때 0.1스텝으로 학습시켰을 때 정확도는 89퍼 정도 되었는데\\n\\nwide and deep 설계한 후 정확도가 48~55퍼로 감소함.\\n\\n스텝을 0.3까지 늘리니 68~77퍼로 증가하였지만 0.4스텝으로 늘렸을 때 다시 60퍼센트로 감소함.\\n\\n출력을 10배 , 6배 , 1배로 설계하였더니\\n\\n0.4스텝에서 78퍼~81퍼로 정확도가 올라감\\n0.1스텝에서 47퍼. underfitting.\\n\\n신경망 볼륨이 올라갈 수 록 스텝도 늘려야 할 것 같다.\\n\\n문제 발견. 활성함수로 시그모이드를 사용해서 배니싱그레디언트현상 때문\\n\\n깊어질 수록 배니싱그레이언트 현상이 발생함.\\n\\n은닉층 하나를 줄이고 10배 , 1배로 설계한 후 0.1스텝으로 학습 -> 정확도 86퍼센트\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#데이터 준비\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "#학습 모델 준비\n",
    "X = tf.placeholder(tf.float32, [None, 784], \"X\")\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes], \"Y\")\n",
    "\n",
    "with tf.name_scope(\"Layer1\"):\n",
    "    W1 = tf.Variable(tf.random_normal([784, nb_classes * 10]), name=\"weight_1\")\n",
    "    b1 = tf.Variable(tf.random_normal([nb_classes * 10]), name=\"bias_1\")\n",
    "    layer1 = tf.nn.softmax(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Layer1\", layer1)\n",
    "    \n",
    "with tf.name_scope(\"Layer2\"):\n",
    "    W2 = tf.Variable(tf.random_normal([nb_classes * 10, nb_classes]), \"weight_2\")\n",
    "    b2 = tf.Variable(tf.random_normal([nb_classes]), \"bias_2\")\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(layer1, W2) + b2)\n",
    "    \n",
    "    tf.summary.histogram(\"W2\", W2)\n",
    "    tf.summary.histogram(\"b2\", b2)\n",
    "    tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) \n",
    "    tf.summary.scalar(\"Cost\", cost)\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate=3e-1).minimize(cost)\n",
    "\n",
    "\n",
    "#모델 성능 평가 준비\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "#파라미터\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/MNIST_wide_and_deep\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph4\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    #학습 사이클\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0;\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, summary, cost_val = sess.run([train, merged_summary, cost], feed_dict={X:batch_xs, Y:batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "            writer.add_summary(summary, global_step=i)\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "    \n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    #테스트 데이터로 모델 테스트\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X:mnist.test.images, Y:mnist.test.labels}), )\n",
    "\n",
    "    #1개 추출 , 예측\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X:mnist.test.images[r:r+1]}), )\n",
    "\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap=\"Greys\", interpolation=\"nearest\", )\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "은닉층을 2개 늘리고\n",
    "\n",
    "출력은 4배에서 2배 1배로 출력되도록 설계함\n",
    "\n",
    "은닉층이 없을 때 0.1스텝으로 학습시켰을 때 정확도는 89퍼 정도 되었는데\n",
    "\n",
    "wide and deep 설계한 후 정확도가 48~55퍼로 감소함.\n",
    "\n",
    "스텝을 0.3까지 늘리니 68~77퍼로 증가하였지만 0.4스텝으로 늘렸을 때 다시 60퍼센트로 감소함.\n",
    "\n",
    "출력을 10배 , 6배 , 1배로 설계하였더니\n",
    "\n",
    "0.4스텝에서 78퍼~81퍼로 정확도가 올라감\n",
    "0.1스텝에서 47퍼. underfitting.\n",
    "\n",
    "신경망 볼륨이 올라갈 수 록 스텝도 늘려야 할 것 같다.\n",
    "\n",
    "문제 발견. 활성함수로 시그모이드를 사용해서 배니싱그레디언트현상 때문\n",
    "\n",
    "깊어질 수록 배니싱그레이언트 현상이 발생함.\n",
    "\n",
    "은닉층 하나를 줄이고 10배 , 1배로 설계한 후 0.1스텝으로 학습 -> 정확도 86퍼센트\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.826302672\n",
      "Epoch: 0002, Cost: 1.061668952\n",
      "Epoch: 0003, Cost: 0.838061315\n",
      "Epoch: 0004, Cost: 0.733232745\n",
      "Epoch: 0005, Cost: 0.669279885\n",
      "Epoch: 0006, Cost: 0.624611836\n",
      "Epoch: 0007, Cost: 0.591160344\n",
      "Epoch: 0008, Cost: 0.563868987\n",
      "Epoch: 0009, Cost: 0.541745171\n",
      "Epoch: 0010, Cost: 0.522673578\n",
      "Epoch: 0011, Cost: 0.506782325\n",
      "Epoch: 0012, Cost: 0.492447643\n",
      "Epoch: 0013, Cost: 0.479955837\n",
      "Epoch: 0014, Cost: 0.468893674\n",
      "Epoch: 0015, Cost: 0.458703488\n",
      "Learning finished\n",
      "Accuracy:  0.8951\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#데이터 준비\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "#학습 모델 준비\n",
    "X = tf.placeholder(tf.float32, [None, 784], \"X\")\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes], \"Y\")\n",
    "\n",
    "with tf.name_scope(\"Hypothesis\"):\n",
    "    W1 = tf.Variable(tf.random_normal([784, nb_classes]), name=\"weight_1\")\n",
    "    b1 = tf.Variable(tf.random_normal([nb_classes]), name=\"bias_1\")\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    tf.summary.histogram(\"W1\", W1)\n",
    "    tf.summary.histogram(\"b1\", b1)\n",
    "    tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
    "    \n",
    "with tf.name_scope(\"Cost\"):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) \n",
    "    tf.summary.scalar(\"Cost\", cost)\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(cost)\n",
    "\n",
    "\n",
    "#모델 성능 평가 준비\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "#파라미터\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard --logdir=./logs/xor_logs\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"./logs/MNIST_base_5\")\n",
    "    writer.add_graph(sess.graph)  # Show the graph4\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "    #학습 사이클\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0;\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, summary, cost_val = sess.run([train, merged_summary, cost], feed_dict={X:batch_xs, Y:batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "        \n",
    "        writer.add_summary(summary, global_step=epoch)\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "    \n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    #테스트 데이터로 모델 테스트\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X:mnist.test.images, Y:mnist.test.labels}), )\n",
    "\n",
    "    #1개 추출 , 예측\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X:mnist.test.images[r:r+1]}), )\n",
    "\n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28, 28), cmap=\"Greys\", interpolation=\"nearest\", )\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
